#!/usr/bin/env python
'''
produce SGE SNV scores from files of variant-, replicate-, and timepoint-specific
integer counts
'''

import sys
import argparse
import pandas as pd
import numpy as np
import os
import string

sys.path.append("/net/bbi/vol1/data/sge-analysis/lib/")
import sge_util
import sge_counts
import sge_target

import loess.loess_1d
from scipy.stats import linregress
from scipy.stats import norm
from sklearn.mixture import GaussianMixture


# Function to assign the most important SO summary term
def get_simplified_consequence(vep_terms, ensemblfile):
    consequence_df = pd.read_csv(ensemblfile, sep='\t')
    # Create a dictionary mapping terms to their SO summary term (prioritizing importance)
    mapping_dict = (
        consequence_df.sort_values(by="Importance", ascending=False)
        .groupby("VEP output term")[["SO summary term", "Importance"]]
        .first()
        .to_dict()["SO summary term"]
    )
    if not isinstance(vep_terms, str):  
        return "Unknown"
    terms = vep_terms.split(',')  
    matched_terms = [mapping_dict.get(term.strip()) for term in terms if term.strip() in mapping_dict]
    
    return matched_terms[0] if matched_terms else "Unknown"


def find_nearest(array, value):
    array = np.asarray(array)
    idx = (np.abs(array - value)).argmin()
    return [array[idx], idx]


def makeFunctionalClasses(args, scoredf, exclude_nonsense_coords):
    mean_nonsense = scoredf[(scoredf["consequence"].isin(["stop_gained",])) &
                            (scoredf["variant_qc_flag"] == "PASS") &
                            (scoredf["variant_type"] == "snv") &
                            (~scoredf["pos"].isin(exclude_nonsense_coords))]["score"].mean()
    syn_low = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                      (scoredf["variant_type"] == "snv") &
                      (scoredf["variant_qc_flag"] == "PASS")]["score"].quantile(0.025)
    syn_high = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                       (scoredf["variant_type"] == "snv") &
                       (scoredf["variant_qc_flag"] == "PASS")]["score"].quantile(0.975)
    mean_syn = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                       (scoredf["variant_qc_flag"] == "PASS") &
                       (scoredf["variant_type"] == "snv") &
                       (scoredf["score"] >= syn_low) & (scoredf["score"] <= syn_high)]["score"].mean()
    syn_fit = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                      (scoredf["variant_type"] == "snv") &
                      (scoredf["variant_qc_flag"] == "PASS") &
                      (scoredf["score"] >= syn_low) & (scoredf["score"] <= syn_high)]["score"].to_list()

    nons = scoredf[(scoredf["consequence"].isin(["stop_gained",])) &
                   (scoredf["variant_type"] == "snv") &
                   (scoredf["variant_qc_flag"] == "PASS") &
                   (~scoredf["pos"].isin(exclude_nonsense_coords))]["score"].to_list()
    nons.extend(syn_fit)
    gm = GaussianMixture(
        n_components=2, 
        means_init=np.array([mean_nonsense, mean_syn]).reshape(-1, 1), 
        random_state=args.randomseed
    ).fit(
        np.array(nons).reshape(-1, 1)
    )
    if args.verbose:
        sys.stderr.write("INFO: would classify %d variants before NaN filtering\n" % scoredf.shape[0])
    scoresub = scoredf[~scoredf["score"].isna()]
    if args.verbose:
        sys.stderr.write("INFO: after filtering, will classify %d variants\n" % scoresub.shape[0])
    allscores = np.array(scoresub["score"].to_list()).reshape(-1, 1)
    preds = gm.predict_proba(allscores)
    scoresub[["gmm_density_abnormal", "gmm_density_normal"]] = preds
    #scoredf[["gmm_density_abnormal", "gmm_density_normal"]] = preds
    scoresub["functional_consequence"] = "indeterminate"
    scoresub.loc[scoresub["gmm_density_abnormal"] >= args.gmmthresh, "functional_consequence"] = "functionally_abnormal"
    scoresub.loc[scoresub["gmm_density_normal"] >= args.gmmthresh, "functional_consequence"] = "functionally_normal"
    scoredf = scoredf.merge(scoresub, how="left")
    # avoid the counter-intuitive situation where some high-scoring variants could get abnormal labels owing
    # to heavy tails in the abnormal distribution
    normalmax = scoredf[(scoredf["functional_consequence"] == "functionally_normal") &
                        (scoredf["variant_qc_flag"] == "PASS")]["score"].max()
    scoredf.loc[scoredf["score"] >= normalmax, "functional_consequence"] = "functionally_normal"

    # get the location and scale of the estimated GMM components
    mean_abnormal = gm.means_[0][0]
    mean_normal = gm.means_[1][0]
    sd_abnormal = np.sqrt(gm.covariances_[0][0][0])
    sd_normal = np.sqrt(gm.covariances_[1][0][0])

    # now we try to approximate the thresholds 
    diff_normal = (scoredf['gmm_density_normal'] - args.gmmthresh).abs()
    closest_index = diff_normal.idxmin()
    closest_row_normal = scoredf.loc[closest_index]

    diff_abnormal = (scoredf['gmm_density_abnormal'] - args.gmmthresh).abs()
    closest_index = diff_abnormal.idxmin()
    # Retrieve the row with the closest value
    closest_row_abnormal = scoredf.loc[closest_index]
    # now we get the scores that are the closest to the normal and abnormal thresholds
    closest_score_normal = closest_row_normal['score']
    closest_score_abnormal = closest_row_abnormal['score']
    if args.verbose:
        sys.stderr.write("INFO: abnormal score threshold starting estimate: %f\n" % closest_score_abnormal)
        sys.stderr.write("INFO: normal score threshold starting estimate: %f\n" % closest_score_normal)

    # generate and score a bunch of tightly-spaced candidates
    candidates_abnormal = np.linspace(closest_score_abnormal - 0.01, closest_score_abnormal + 0.01, 10000)
    preds_abnormal = gm.predict_proba(candidates_abnormal.reshape(-1, 1))
    closest_density, index = find_nearest(preds_abnormal[:, 0], args.gmmthresh)
    thresh_abnormal = candidates_abnormal[index]
    if args.verbose:
        sys.stderr.write("INFO: best abnormal score threshold = %f resulting in density %f\n" % (thresh_abnormal, closest_density))
    
    # generate and score a bunch of tightly-spaced candidates
    candidates_normal = np.linspace(closest_score_normal - 0.01, closest_score_normal + 0.01, 10000)
    preds_normal = gm.predict_proba(candidates_normal.reshape(-1, 1))
    closest_density, index = find_nearest(preds_normal[:, 1], args.gmmthresh)
    thresh_normal = candidates_normal[index]
    if args.verbose:
        sys.stderr.write("INFO: best normal score threshold = %f resulting in density %f\n" % (thresh_normal, closest_density))

    if args.outputmodel:
        saveModelParameters(
            args,
            mean_abnormal,
            mean_normal,
            sd_abnormal,
            sd_normal,
            thresh_abnormal,
            thresh_normal
        )

    return scoredf


def saveDeletionCounts(args, df):
    countcols = [c for c in df.columns if c.startswith("D") and c[-2:] in ("R1", "R2", "R3")]
    outcols = ["chrom", "start", "end", "target", "snvlib_count"]
    outcols.extend(countcols)
    df = df[outcols]
    outfile = args.outputdeletioncounts
    df.to_csv(outfile, index=False, sep="\t")
    if args.verbose:
        sys.stderr.write("INFO: wrote deletion counts to %s\n" % outfile)
    return


def saveSNVCounts(args, df):
    df = df.rename(columns={'allele': 'alt'})
    countcols = [c for c in df.columns if c.startswith("D") and c[-2:] in ("R1", "R2", "R3")]
    outcols = ["chrom", "pos", "ref", "alt", "target", "snvlib_count"]
    outcols.extend(countcols)
    df = df[outcols]
    outfile = args.outputsnvcounts
    df.to_csv(outfile, index=False, sep="\t")
    if args.verbose:
        sys.stderr.write("INFO: wrote SNV counts to %s\n" % outfile)
    return


def saveModelParameters(args, mean_abnormal, mean_normal, sd_abnormal, sd_normal, thresh_abnormal, thresh_normal):
    with open(args.outputmodel, "w") as outfile:
        outfile.write("gmm_thresh\tmean_abnormal\tmean_normal\tsd_abnormal\tsd_normal\tthresh_abnormal\tthresh_normal\trandom_seed\n")
        outfile.write("%f\t%f\t%f\t%f\t%f\t%f\t%f\t%d\n" % (
            args.gmmthresh,
            mean_abnormal,
            mean_normal,
            sd_abnormal,
            sd_normal,
            thresh_abnormal,
            thresh_normal,
            args.randomseed)
        )            
    if args.verbose:
        sys.stderr.write("INFO: wrote model parameters to %s\n" % args.outputmodel)
    return


def saveScores(args, df):
    outfile = args.outfile
    df.to_csv(outfile, index=False, sep="\t", float_format='%g')
    if args.verbose:
        sys.stderr.write("INFO: wrote scores to %s\n" % outfile)
    return
                  

def getAnnots(args, vepdir, targetlist, targetfile, type="snv"):
    '''read the annotations from the VEP output for each tile, and return a dataframe
    '''
    # create a set to hold protein positions where we want to apply a warning
    warning_pos = set()
    if args.verbose:
        sys.stderr.write("INFO: Loading annotation files for variants from %s\n" % vepdir)
    annotdf = pd.DataFrame()
    for target in targetlist:
        if type == "snv":
            vepfile = os.path.join(vepdir, "%s.snvs.vep.tsv" % target)
        else:
            vepfile = os.path.join(vepdir, "%s.dels.vep.tsv" % target)
        vepdf = sge_util.getVEPdf(vepfile, type=type)
        vepdf["amino_acid_change"] = vepdf.apply(sge_util.makeAAsub, axis=1)
        if type == "snv":
            vepdf[["chrom", "pos"]] = vepdf["Location"].str.split(":", expand=True)
            vepdf["start"] = pd.NA
            vepdf["end"] = pd.NA
            vepdf["pos_id"] = vepdf["pos"] + ":" + vepdf["allele"]

            t = sge_target.Target(target, targetfile)
            for pos in t.required_edits:
                protein_pos = vepdf[vepdf["pos"] == str(pos)]["Protein_position"].values[0]
                if protein_pos != '-':
                    warning_pos.add(str(protein_pos))
                    if args.verbose:
                        sys.stderr.write("INFO: identified required edit at protein position %d\n" % int(protein_pos))
            for pos in t.skip_pos:
                protein_pos = vepdf[vepdf["pos"] == str(pos)]["Protein_position"].values[0]
                if protein_pos != '-':
                    warning_pos.add(str(protein_pos))
                    if args.verbose:
                        sys.stderr.write("INFO: identified skipped position (cell line SNV?) at protein position %d\n"
                                        % int(protein_pos))
            annotdf = pd.concat(
                [
                    annotdf,
                    vepdf[
                        ["chrom", "pos", "start", "end",
                         "allele",
                         "pos_id", "amino_acid_change",
                         "Consequence", "hgvs_p", "hgvs_g",
                         "CDS_position", "cDNA_position",
                         "Protein_position"]
                    ]
                ]
            )

        else:
            # read the skeleton VCF we supplied to VEP
            vepskel = os.path.join(vepdir, "%s.dels.skeleton.vcf" % target)
            vepskeldf = pd.read_csv(vepskel, sep="\t", skiprows=1)
            vepskeldf["start"] = vepskeldf["POS"] + 1
            vepdf = vepdf.merge(
                vepskeldf[["POS", "start", "REF", "ALT"]], 
                on="start"
            ).rename(
                columns={
                    'POS': 'pos',
                    'REF': 'ref',
                    'ALT': 'allele'
                }
            )
            vepdf["pos_id"] = pd.NA           
            annotdf = pd.concat(
                [
                    annotdf,
                    vepdf[
                        ["chrom", "pos", "start", "end",
                         "ref", "allele", 
                         "pos_id", "amino_acid_change", 
                         "Consequence", "hgvs_p", "hgvs_g",
                         "CDS_position", "cDNA_position"
                         ]
                    ]   
                ]   
            )
    annotdf["warn_codon"] = ""
    if type == "snv":
        annotdf.loc[annotdf["Protein_position"].astype(str).isin(warning_pos), "warn_codon"] = "WARN"
        annotdf = annotdf.drop_duplicates(
            subset=["pos_id"]
        ).drop(
            columns=["Protein_position"]
        )
   
    else:
        annotdf = annotdf.drop_duplicates()

    return annotdf


def runModel(row, days):
    '''apply the simple scoring model to each row in the dataframe, 
    return the score and standard error of the score

    thie model is the same for both deletions and SNVs
    '''
    x = []
    y = []
    colnames = list(row.index)

    for d in days:
        cols = [c for c in colnames if c.startswith(d) and c.endswith("_log2_adjusted")]
        for c in cols:
            #if not pd.isna(row[c]):
            if np.isfinite(row[c]):
                x.append(int(d[1:]))
                y.append(row[c])
            else:
                sys.stderr.write("Filtering non-finite element\n")
    x.append(0)
    y.append(0)
    x=np.array(x)
    y=np.array(y)
    
    try:
        model = linregress(x, y)
    except Exception as e:
        #print(f"Caught error scoring {row['start']}: {e}")
        return [None, None]
    
    return [model.slope, model.stderr]


def runModelDups(row, days):
    '''for variants that overlap in two adjacent tiles, run the model
    using data from both tiles to produce a unified score. return the
    score and the standard error of the score
    '''
    x = []
    y = []
    colnames = list(row.index)

    for d in days:
        cols = [c for c in colnames if c.startswith(d) and c.endswith("_log2_adjusted")]
        for c in cols:
            for myvalue in row[c]:
                #if not pd.isna(myvalue):
                if np.isfinite(myvalue):
                    x.append(int(d[1:]))
                    y.append(myvalue)
                else:
                    sys.stderr.write("Filtering non-finite element\n")

    x.append(0)
    y.append(0)
    x=np.array(x)
    y=np.array(y)
    try:
        model = linregress(x, y)
    except:
        return [None, None]
    return [model.slope, model.stderr]


def handleDups(args, countsdf, dupdf, dupid):
    # dupid is one of "start" or "pos_id"
    if args.verbose:
        sys.stderr.write("INFO: Collapsing scores of duplicated variants\n")
    filtdf = countsdf[countsdf[dupid].isin(dupdf[dupid])]
    if args.verbose:
        sys.stderr.write("INFO: identified %d duplicate entries\n" % filtdf.shape[0])
    tmpdf = filtdf.groupby(
        [dupid]
    ).agg(
        list
    ).reset_index()
    tmpdf[['dupscore', 'dupse']] = tmpdf.apply(
        runModelDups,
        axis=1,
        result_type='expand',
        days=args.daystrings
    )
    return tmpdf[[dupid, "dupscore", "dupse"]]


def scoreSNVTarget(args, tdf, t):
    if args.verbose:
        sys.stderr.write("INFO: Scoring SNVs in %s\n" % t)

    days = args.daystrings
    tdf[['modelscore', 'modelse']] = tdf.apply(
        runModel,
        axis=1,
        result_type='expand',
        days=days
    )
    resdf = tdf[[
        "target", "exon", "chrom", "pos", "allele", "ref", "pos_id", "modelscore", "modelse"
    ]]
    return resdf


def scoreDeletionTarget(args, tdf, t):
    if args.verbose:
        sys.stderr.write("INFO: Scoring deletions in %s\n" % t)

    days = args.daystrings
    tdf[['modelscore', 'modelse']] = tdf.apply(
        runModel,
        axis=1,
        result_type='expand',
        days=days
    )
    resdf = tdf[[
        "target", "exon", "chrom", "start",
        "end", "modelscore", "modelse"
    ]]
    return resdf


# use a loess smoother to smooth the non-day5 frequencies
def smootheSNVFreqs(args, df, daycounts):
    filtercols = [col for col in df.columns if col.startswith("D05_") \
                  and col.endswith("over_snvlib")]
    cond = (df[filtercols[0]] >= 0.5)
    if len(filtercols) > 1:
        for col in filtercols[1:]:
            cond &= (df[col] >= 0.5)

    fullcount = df.shape[0]
    filtdf = df[cond]
    filtcount = filtdf.shape[0]

    allcoords = df["pos"].unique()
    if args.verbose:
        sys.stderr.write("INFO: smoothing started with %d variants\n" % fullcount)
        sys.stderr.write("INFO: after filtering with threshold=%f, %d variants remain\n" % (0.5,
                                                                           filtcount))

    for col in filtercols:
        loesscol = col.replace("over_snvlib", "loess_adjustment")
        xlist = filtdf['pos'].to_numpy()
        ylist = filtdf[col].to_numpy()
        ylist = np.log2(ylist)
        xout, yout, wout = loess.loess_1d.loess_1d(
            xlist,
            ylist,
            xnew=allcoords,
            frac=0.10
        )
        mydata = np.array([xout, yout])
        tmpdf = pd.DataFrame(
            data=mydata.T, 
            columns=["pos", loesscol]
        ).drop_duplicates(
            subset=['pos'], 
            keep='first'
        )
        tmpdf['pos'] = tmpdf['pos'].astype(int)

        df = df.merge(tmpdf, on="pos")

        for d in daycounts:
            origcol = col.replace("D05", d) + "_log2"
            newcol = origcol + "_adjusted"
            df[newcol] = df[origcol] - df[loesscol]

    return df


def smootheDeletionFreqs(args, df, daycounts):
    filtercols = [col for col in df.columns if col.startswith("D05_") \
                  and col.endswith("over_snvlib")]
    cond = (df[filtercols[0]] >= 0)
    if len(filtercols) > 1:
        for col in filtercols[1:]:
            cond &= (df[col] >= 0)

    fullcount = df.shape[0]
    filtdf = df[cond]
    filtcount = filtdf.shape[0]

    allcoords = df["start"].unique()
    if args.verbose:
        sys.stderr.write("INFO: smoothing started with %d dels\n" % fullcount)
        sys.stderr.write("INFO: after filtering with threshold=%f, %d dels remain\n" % (0.5,
                                                                           filtcount))

    for col in filtercols:
        loesscol = col.replace("over_snvlib", "loess_adjustment")
        xlist = filtdf['start'].to_numpy()
        ylist = filtdf[col].to_numpy()
        ylist = np.log2(ylist)
        xout, yout, wout = loess.loess_1d.loess_1d(
            xlist,
            ylist,
            xnew=allcoords,
            frac=0.2
        )
        mydata = np.array([xout, yout])
        tmpdf = pd.DataFrame(
            data=mydata.T, 
            columns=["start", loesscol]
        ).drop_duplicates(
            subset=['start'], 
            keep='first'
        )
        tmpdf['start'] = tmpdf['start'].astype(int)

        df = df.merge(tmpdf, on="start")

        for d in daycounts:
            origcol = col.replace("D05", d) + "_log2"
            newcol = origcol + "_adjusted"
            df[newcol] = df[origcol] - df[loesscol]

    return df


def filterCounts(args, tdf, t, all_countcols, all_freqcols):
    if args.verbose:
        sys.stderr.write("INFO: filtering counts for target %s\n" % t)
        sys.stderr.write("INFO: before filtering, dataframe has %d variants\n" % tdf.shape[0])


    freqcols = [col for col in all_freqcols if col[:3] in args.filterstrings] # and col.endswith("freq")]
    countcols = [col for col in all_countcols if col[:3] in args.filterstrings]# and not col.endswith("freq")]

    minfreq = args.minfreq
    mincount = args.mincount
    cnd = (tdf["target"] == t)
    for fcol in freqcols:
        cnd &= (tdf[fcol] >= minfreq)
    for ccol in countcols:
        cnd &= (tdf[ccol] >= mincount)
    tdf = tdf[cnd]

    if args.verbose:
        sys.stderr.write("INFO: after filtering, dataframe has %d variants\n" % tdf.shape[0])
    return tdf


def loadDeletionCounts(args, targetfile, t):
    if args.verbose:
        sys.stderr.write("INFO: working on target %s\n" % t)

    target = sge_target.Target(t, targetfile)
    countcols = []
    freqcols = []
    # load the day 0 library

    delfiles = target.getDelSampleList(
        args.countsdir,
        include_neg=False
    )
    libfile = delfiles['D00'][0]
    df = sge_counts.getDelCounts(
        libfile,
        augment=True,
        pseudocount=0
    )
    df = df.rename(
        columns={'count': 'snvlib_count'}
    ).drop(
        columns=["sampleid", "repl", "day"]
    )

    statsfile = libfile.replace(
        ".dels.tsv",
        ".readstats.tsv"
    )
    statsdf = sge_counts.getReadStats(
        statsfile,
        augment=False
    )
    freqcol = "snvlib_freq"
    normval = statsdf["deletion_reads"][0] + statsdf["snv_reads"][0]
    df[freqcol] = df["snvlib_count"] / normval

    # load the various timepoints and replicates
    nsamples = 1
    daycounts = {}

    for delday, delfilelist in sorted(delfiles.items()):
        if delday == "D00": continue  # already loaded
        shortday = int(delday[1:])
        if shortday not in args.days:
            sys.stderr.write("WARN: Skipping data for day %d as instructed\n" % shortday)
            continue

        for delfile in sorted(delfilelist):
            # exclude neg ctrl
            if "_NC_" in delfile:
                continue
            nsamples += 1
            if delday in daycounts:
                daycounts[delday] += 1
            else:
                daycounts[delday] = 1

            if args.verbose:
                sys.stderr.write("INFO: loading data from %s\n" % delfile)
            tmpdf = sge_counts.getDelCounts(
                delfile,
                augment=False,
                pseudocount=0
            )
            statsfile = delfile.replace(
                ".dels.tsv",
                ".readstats.tsv"
            )
            statsdf = sge_counts.getReadStats(
                statsfile,
                augment=False
            )
            countcol = "%s_R%d" % (delday, daycounts[delday])
            tmpdf = tmpdf.rename(
                columns={'count': countcol}
            )
            countcols.append(countcol)
            freqcol = countcol + "_freq"
            normval = statsdf["deletion_reads"][0] + statsdf["snv_reads"][0]
            tmpdf[freqcol] = tmpdf[countcol] / normval
            freqcols.append(freqcol)
            df = df.merge(
                tmpdf[["chrom", "start", "end", countcol,
                       freqcol, 
                       ]],
                on=["chrom", "start", "end"]
            )
            libratiocol = "%s_over_snvlib" % (countcol)
            df[libratiocol] = df[freqcol] / df["snvlib_freq"]
            log2libratiocol = "%s_over_snvlib_log2" % (countcol)
            df[log2libratiocol] = np.log2(df[libratiocol])

    if args.verbose:
        sys.stderr.write("INFO: Finished loading data from %d samples\n" % nsamples)
        sys.stderr.write("INFO: Using data from timepoints: %s\n" % (', '.join(list(daycounts.keys()))))

    # add target metadata
    df["target"] = t
    if t[-1] not in string.digits:
        shorttarget = t[:-1]
    else:
        shorttarget = t
    df["exon"] = shorttarget

    # filter low counts/freqs
    df = filterCounts(
        args,
        df,
        t,
        countcols,
        freqcols
    )

    # smoothe the freqs
    df = smootheDeletionFreqs(
        args,
        df,
        daycounts
    )
    return df



def loadSNVCounts(args, targetfile, t):
    if args.verbose:
        sys.stderr.write("INFO: working on target %s\n" % t)

    target = sge_target.Target(t, targetfile)
    countcols = []
    freqcols = []
    # load the day 0 library
    snvfiles = target.getSNVSampleList(args.countsdir, include_neg=False)
    libfile = snvfiles['D00'][0]
    df = sge_counts.getSNVCounts(
        libfile,
        augment=True,
        pseudocount=1
    )
    df = df.rename(
        columns={'count': 'snvlib_count'}
    ).drop(
        columns=["sampleid", "repl", "day"]
    )
    statsfile = libfile.replace(".snvs.tsv", ".readstats.tsv")
    statsdf = sge_counts.getReadStats(statsfile, augment=False)
    freqcol = "snvlib_freq"
    normval = statsdf["snv_reads"][0]
    df[freqcol] = df["snvlib_count"] / normval

    # load the various timepoints and replicates
    nsamples = 1
    daycounts = {}

    for snvday, snvfilelist in sorted(snvfiles.items()):
        if snvday == "D00": continue  # already loaded
        shortday = int(snvday[1:])
        if shortday not in args.days:
            sys.stderr.write("WARN: Skipping data for day %d as instructed\n" % shortday)
            continue

        for snvfile in sorted(snvfilelist):
            # exclude neg ctrl
            if "_NC_" in snvfile:
                continue
            nsamples += 1
            if snvday in daycounts:            
                daycounts[snvday] += 1
            else:
                daycounts[snvday] = 1

            if args.verbose:
                sys.stderr.write("INFO: loading data from %s\n" % snvfile)
            tmpdf = sge_counts.getSNVCounts(
                snvfile,
                augment=False,
                pseudocount=1
            )
            statsfile = snvfile.replace(".snvs.tsv", ".readstats.tsv")
            statsdf = sge_counts.getReadStats(statsfile, augment=False)
            countcol = "%s_R%d" % (snvday, daycounts[snvday])
            tmpdf = tmpdf.rename(columns={'count': countcol})
            countcols.append(countcol)
            freqcol = countcol + "_freq"
            normval = statsdf["snv_reads"][0]
            tmpdf[freqcol] = tmpdf[countcol] / normval
            freqcols.append(freqcol)
            df = df.merge(
                tmpdf[["chrom", "pos", "allele", countcol,
                       freqcol  
                       ]],
                on=["chrom", "pos", "allele"]
            )
            libratiocol = "%s_over_snvlib" % (countcol)
            df[libratiocol] = df[freqcol] / df["snvlib_freq"]
            log2libratiocol = "%s_over_snvlib_log2" % (countcol)
            df[log2libratiocol] = np.log2(df[libratiocol])

    if args.verbose:
        sys.stderr.write("INFO: Finished loading data from %d samples\n" % nsamples)
        sys.stderr.write("INFO: Using data from timepoints: %s\n" % (', '.join(list(daycounts.keys()))))

    # filter the counts for only the edited bases
    df = df[(df["pos"] >= target.editstartpos) &
            (df["pos"] <= target.editendpos)]

    # merge with the reference sequence
    df = df.merge(target.refdf, on="pos")
    df = df[df["ref"] != df["allele"]]
    df["pos_id"] = df["pos"].astype(str) + ":" + df["allele"]
    df = df[~df["pos"].isin(target.required_edits)]
    df = df[~df["pos"].isin(target.skip_pos)]

    # add target metadata
    df["target"] = t
    if t[-1] not in string.digits:
        shorttarget = t[:-1]
    else:
        shorttarget = t
    df["exon"] = shorttarget

    # filter low counts/freqs
    df = filterCounts(
        args,
        df,
        t,
        countcols,
        freqcols
    )

    # smoothe the freqs
    df = smootheSNVFreqs(
        args,
        df,
        daycounts
    )

    return df


def main():
    parser = argparse.ArgumentParser(
        prog='scoreVariants',
        description='produce SGE SNV and deletion scores from counts for a set of tiles in a gene'
    )
    parser.add_argument(
        '-g', '--gene', required=True,
        help='Gene to score'
    )
    parser.add_argument(
        '-c', '--countsdir', required=True,
        help='Load counts files from <countsdir>'
    )
    parser.add_argument(
        '--nodels',  required=False, default=False,
        action="store_true", 
        help="Do not score deletions (default=False)"
    )
    parser.add_argument(
        '--nosnvs',  required=False, default=False,
        action="store_true", 
        help="Do not score SNVs (default=False)"
    )
    parser.add_argument(
        '-o', '--outfile', required=True,
        help="Path to output TSV file of scores"
    )
    parser.add_argument(
        '-C', '--outputsnvcounts', required=False, default="",
        help="Path to output TSV file of per-target SNV counts for each replicate"
    )
    parser.add_argument(
        '-D', '--outputdeletioncounts', required=False, default="",
        help="Path to output TSV file of per-target deletion counts for each replicate"
    )
    parser.add_argument(
        '-E', '--outputmodel', required=False, default="",
        help="Path to output TSV file of GMM component means, variances, and class thresholds"
    )    

    parser.add_argument(
        '-V', '--vepdir', required=False,
        help="Path to directory with VEP files"
    )
    parser.add_argument(
        '-e', '--ensemblfile', required=False,
        default="/net/bbi/vol1/data/sge-analysis/etc/extended_ensembl_consequence.tsv",
        help="Path to ensembl consequence groupings in TSV format"
    )
    parser.add_argument(
        '-t', '--targetfile', required=False,
        help='Path to targets.tsv file (can be auto detected)'
    )
    parser.add_argument(
        '-v', '--verbose', required=False, default=False,
        action="store_true", help="Verbose output"
    )
    parser.add_argument(
        '-x', '--exclude', required=False,
        help='Exclude these regions when scoring a gene (comma-sep list)'
    )
    parser.add_argument(
        '-X', '--nononsense', required=False,
        help="Exclude nonsense variants in these targets when assigning functional classes"
    )
    parser.add_argument(
        '-m', '--mincount', required=False, type=int, default=0,
        help='filter out variants with raw counts less than <mincount> in any replicate'
    )
    parser.add_argument(
        '-M', '--minfreq', required=False, type=float, default=0.,
        help='filter out variants with frequencies less than <minfreq> in any replicate'
    )
    parser.add_argument(
        '-f', '--filter', required=False, type=str,
        help="Comma-separated list of days to use in count filtering (e.g., -f 5 or -f 5,13,17 )"
    )
    parser.add_argument(
        '-d', '--days', required=True, type=str,
        help="Comma-separated list of days to use in model (e.g., -d 5,13 or -d 5,13,17 )"
    )
    parser.add_argument(
        '-r', '--randomseed', required=False, default=54321, type=int,
        help="Random seed (integer) for GMM estimation algorithm (default 54321)"
    )
    parser.add_argument(
        '-T', '--gmmthresh', required=False, default=0.95, type=float,
        help="Minimum estimated GMM component density to assign functional class (def. 0.95)"
    )
    parser.add_argument(
        '-F', '--minfunc', required=False, default=1000, type=int,
        help="Minimum number of scored variants required before functional class assignment"
    )
                        
    args = parser.parse_args()
    snv_mode = True
    if args.nosnvs:
        sys.stderr.write("INFO: Deactivating SNV scoring due to --nosnvs flag\n")
        snv_mode = False

    deletion_mode = True
    if args.nodels:
        sys.stderr.write("INFO: Deactivating deletion scoring due to --nodels flag\n")
        deletion_mode = False
    
    if not snv_mode and not deletion_mode:
        sys.stderr.write("ERROR: must score either deletions or SNVs, or both\n")
        sys.exit(-99)

    # ------------------------------------------------------------
    # command-line argument management
    # ------------------------------------------------------------

    # handle the targetfile
    if not args.targetfile:
        targetfile = sge_util.guess_target_file(args.gene)
    else:
        targetfile = args.targetfile
    if not targetfile or not os.path.exists(targetfile):
        sys.stderr.write("ERROR: Can't locate target file\n")
        sys.exit(-99)

    # handle the VEP output directory
    if not args.vepdir:
        vepdir = "/net/bbi/vol1/data/sge-analysis/etc/%s/" % args.gene
    else:
        vepdir = args.vepdir
    if not os.path.isdir(vepdir):
        sys.stderr.write("ERROR: Can't find VEP directory %s\n" % vepdir)
        sys.exit(-99)

    # what days to use for scoring
    if not args.days:
        sys.stderr.write("ERROR: Must supply -d / --days\n")
        sys.exit(-99)

    try:
        args.days = [int(s.strip()) for s in args.days.split(",")]
        # turn [5, 13, 17] into ["D05", "D13", "D17"]
        args.daystrings = ["D" + str(d).zfill(2) for d in args.days]
        sys.stderr.write("INFO: using days %s in model\n" % ', '.join(map(str, args.days)))

    except:
        sys.stderr.write("ERROR: Unable to parse days from -d / --days argument: %s\n" % args.days)
        sys.exit(-99)

    # parse the filtering days
    if not args.filter:
        sys.stderr.write("INFO: No count-based filtering will be applied\n")
        args.filter = []
        args.filterstrings = []

    else:
        try:
            args.filter = [int(s.strip()) for s in args.filter.split(",")]
            # turn [5, 13, 17] into ["D05", "D13", "D17"]
            args.filterstrings = ["D" + str(d).zfill(2) for d in args.filter]

            sys.stderr.write("INFO: will filter based on days %s\n" % ', '.join(map(str, args.filter)))

        except:
            sys.stderr.write("ERROR: Unable to parse days from -f / --filter argument: %s\n" % args.filter)
            sys.exit(-99)

    # load the target file and exclude any targets if necessary
    targetdf = pd.read_csv(targetfile, sep="\t", header=0)
    targetlist = targetdf["target"].to_list()

    if args.verbose:
        sys.stderr.write(
            "INFO: Found %d targets in targetfile %s\n" % (
                len(targetlist),
                targetfile
            )
        )

    # check for excluded targets
    exclude_targets = []
    if args.exclude:
        parts = args.exclude.split(",")
        for p in parts:
            exclude_targets.append(p)
    targetlist = [tl for tl in targetlist if tl not in exclude_targets]
    if args.verbose:
        sys.stderr.write("INFO: After processing exclusions, %d targets remain\n" % (len(targetlist)))

    # check for targets to exclude from GMM fitting
    exclude_nonsense = []
    exclude_nonsense_coords = set()
    if args.nononsense:
        parts = args.nononsense.split(",")
        for p in parts:
            exclude_nonsense.append(p)
    for target in exclude_nonsense:
        edit_start_pos = targetdf.loc[targetdf["target"] == target, "editstart"].values[0]
        edit_stop_pos = targetdf.loc[targetdf["target"] == target, "editstop"].values[0]
        for pos in range(edit_start_pos, edit_stop_pos + 1, 1):
            exclude_nonsense_coords.add(pos)
    if args.verbose:
        sys.stderr.write("INFO: Will exclude %d positions from nonsense threshold computation\n" % len(exclude_nonsense_coords))


    # ------------------------------------------------------------
    # main SNV workflow starts here
    # ------------------------------------------------------------
    if snv_mode:
        snvannotdf = getAnnots(args, vepdir, targetlist, targetfile)
        snvcountsdf = pd.DataFrame()
        snvscoredf = pd.DataFrame()
        for t in targetlist:
            try:
                tdf = loadSNVCounts(args, targetfile, t)
            except:
                sys.stderr.write("WARN: couldn't load SNV counts for target %s, skipping\n" % t)
                continue
            snvcountsdf = pd.concat([snvcountsdf, tdf])
            tscoresdf = scoreSNVTarget(args, tdf, t)
            snvscoredf = pd.concat([snvscoredf, tscoresdf])

        # now done with individual SNV targets, so we need to collapse SNV dups
        snvdupdf = snvscoredf[snvscoredf.duplicated("pos_id", keep=False) == True]
        snvdupscoredf = handleDups(args, snvcountsdf, snvdupdf, "pos_id")

        snvscoredf = snvscoredf.merge(
            snvdupscoredf[["pos_id", "dupscore", "dupse"]],
            on="pos_id", 
            how="outer"
        ).drop_duplicates(
            subset=["pos_id"]
        )
        # if there are scores from the duplicate workflow, use them.
        # otherwise, use the regular scores
        snvscoredf["score"] = snvscoredf["dupscore"]
        snvscoredf["standard_error"] = snvscoredf["dupse"]
        snvscoredf.loc[snvscoredf["score"].isna(), "score"] = snvscoredf["modelscore"]
        snvscoredf.loc[snvscoredf["standard_error"].isna(), "standard_error"] = snvscoredf["modelse"]
        # merge the scores with the variant annotations
        snvscoredf = snvscoredf.merge(
            snvannotdf[[
                "pos_id", 
                "start",
                "end",
                "Consequence",
                "amino_acid_change",
                "hgvs_p",
                "hgvs_g",
                "CDS_position", 
                "cDNA_position",
                "warn_codon"
            ]],
            on="pos_id"
        )


        snvscoredf["variant_type"] = "snv"

    else: # snv_mode is False
        snvscoredf = pd.DataFrame()


    # ------------------------------------------------------------
    # main deletion workflow starts here
    # ------------------------------------------------------------

    if deletion_mode:
        delannotdf = getAnnots(args, vepdir, targetlist, targetfile, type="del")
        
        delcountsdf = pd.DataFrame()
        delscoredf = pd.DataFrame()
        for t in targetlist:
            try:
                tdf = loadDeletionCounts(args, targetfile, t)
            except:
                sys.stderr.write("WARN: couldn't load deletion counts for target %s, skipping\n" % t)
                continue
            delcountsdf = pd.concat([delcountsdf, tdf])
            tscoresdf = scoreDeletionTarget(args, tdf, t)
            delscoredf = pd.concat([delscoredf, tscoresdf])

        # now done with individual deletion targets, so we need to collapse dups
        deldupdf = delscoredf[delscoredf.duplicated("start", keep=False) == True]
        deldupscoredf = handleDups(args, delcountsdf, deldupdf, "start")
        delscoredf = delscoredf.merge(
            deldupscoredf[["start", "dupscore", "dupse"]],
            on="start", 
            how="outer"
        ).drop_duplicates(
            subset=["start"]
        )
        # if there are scores from the duplicate workflow, use them.
        # otherwise, use the regular scores
        delscoredf["score"] = delscoredf["dupscore"]
        delscoredf["standard_error"] = delscoredf["dupse"]
        delscoredf.loc[delscoredf["score"].isna(), "score"] = delscoredf["modelscore"]
        delscoredf.loc[delscoredf["standard_error"].isna(), "standard_error"] = delscoredf["modelse"]

        # now merge with the annotations
        delscoredf = delscoredf.merge(
            delannotdf[[
                "pos", "ref", "allele", 
                "pos_id", "start", "end",
                "Consequence",
                "amino_acid_change",
                "hgvs_p",
                "hgvs_g",
                "CDS_position",
                "cDNA_position",
                "warn_codon"
            ]],
            on=["start", "end"]
        )
        delscoredf["variant_type"] = "deletion"

    else: # deletion_mode is False
        delscoredf = pd.DataFrame()

    # concatenate the scores and annots into one big df
    scoredf = pd.concat([snvscoredf, delscoredf])
    scoredf["95_ci_upper"] = scoredf["score"] + (1.96 * scoredf["standard_error"])
    scoredf["95_ci_lower"] = scoredf["score"] - (1.96 * scoredf["standard_error"])
    scoredf["simplified_consequence"] = scoredf["Consequence"].apply(
        get_simplified_consequence,
        ensemblfile=args.ensemblfile
    )
    scoredf = scoredf.drop(
        columns=["Consequence"]
    ).rename(
        columns={'simplified_consequence': 'consequence',
                 'allele': 'alt'}
    )

    # assign QC flag
    scoredf["variant_qc_flag"] = "PASS"
    scoredf.loc[scoredf["warn_codon"] == "WARN", "variant_qc_flag"] = "WARN"

    if args.outputsnvcounts and snv_mode:
        saveSNVCounts(args, snvcountsdf)

    if args.outputdeletioncounts and deletion_mode:
        saveDeletionCounts(args, delcountsdf)
        

    # ------------------------------------------------------------
    # create functional classes if we have enough variants
    # ------------------------------------------------------------
    if scoredf.shape[0] >= args.minfunc and snv_mode:
        if args.verbose:
            sys.stderr.write(
                "INFO: Dataset has at least %d variants (%d); proceeding with functional class assignment\n" % (
                    args.minfunc,
                    scoredf.shape[0]
                )
            )
        scoredf = makeFunctionalClasses(
            args,
            scoredf,
            exclude_nonsense_coords
        )
    else:
        if args.verbose:
            sys.stderr.write(
                "INFO: Dataset has less than %d variants (%d) or --nosnvs was specified; not performing functional class assignment\n" % (
                    args.minfunc,
                    scoredf.shape[0]
                )
            )
        scoredf["functional_consequence"] = ""
        scoredf["gmm_density_abnormal"] = ""
        scoredf["gmm_density_normal"] = ""


    # ------------------------------------------------------------
    # output
    # ------------------------------------------------------------

    scoredf = scoredf[[
        "chrom", "pos", "ref", "alt", "exon", "target",
        "consequence", "score", 
        "standard_error", "95_ci_upper", "95_ci_lower",
        "amino_acid_change", "hgvs_p", "hgvs_g", "CDS_position", "cDNA_position", 
        "functional_consequence",
        "gmm_density_abnormal", "gmm_density_normal",
        "variant_qc_flag",
    ]]

    saveScores(args, scoredf)
    return 0
    

if __name__ == '__main__':
    main()
