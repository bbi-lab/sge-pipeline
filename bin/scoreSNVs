#!/usr/bin/env python
'''
produce SGE SNV scores from files of variant-, replicate-, and timepoint-specific
integer counts
'''

import sys
import argparse
import pandas as pd
import numpy as np
import os
import string

sys.path.append("/net/bbi/vol1/data/sge-analysis/lib/")
import sge_util
import sge_counts
import sge_target

import loess.loess_1d
from scipy.stats import linregress
from scipy.stats import norm
from sklearn.mixture import GaussianMixture


# Function to assign the most important SO summary term
def get_simplified_consequence(vep_terms, ensemblfile):
    consequence_df = pd.read_csv(ensemblfile, sep='\t')
    # Create a dictionary mapping terms to their SO summary term (prioritizing importance)
    mapping_dict = (
        consequence_df.sort_values(by="Importance", ascending=False)
        .groupby("VEP output term")[["SO summary term", "Importance"]]
        .first()
        .to_dict()["SO summary term"]
    )
    if not isinstance(vep_terms, str):  
        return "Unknown"
    terms = vep_terms.split(',')  
    matched_terms = [mapping_dict.get(term.strip()) for term in terms if term.strip() in mapping_dict]
    
    return matched_terms[0] if matched_terms else "Unknown"


def makeFunctionalClasses(args, scoredf, exclude_nonsense_coords):
    mean_nonsense = scoredf[(scoredf["consequence"].isin(["stop_gained",])) &
                            (scoredf["variant_qc_flag"] == "PASS") &
                            (~scoredf["pos"].isin(exclude_nonsense_coords))]["score"].mean()
    syn_low = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                           (scoredf["variant_qc_flag"] == "PASS")]["score"].quantile(0.025)
    syn_high = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                            (scoredf["variant_qc_flag"] == "PASS")]["score"].quantile(0.975)
    mean_syn = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                       (scoredf["variant_qc_flag"] == "PASS") &
                       (scoredf["score"] >= syn_low) & (scoredf["score"] <= syn_high)]["score"].mean()
    syn_fit = scoredf[(scoredf["consequence"].isin(["synonymous_variant","intron_variant"])) &
                      (scoredf["variant_qc_flag"] == "PASS") &
                      (scoredf["score"] >= syn_low) & (scoredf["score"] <= syn_high)]["score"].to_list()

    nons = scoredf[(scoredf["consequence"].isin(["stop_gained",])) &
                   (scoredf["variant_qc_flag"] == "PASS") &
                   (~scoredf["pos"].isin(exclude_nonsense_coords))]["score"].to_list()
    nons.extend(syn_fit)
    gm = GaussianMixture(
        n_components=2, 
        means_init=np.array([mean_nonsense, mean_syn]).reshape(-1, 1), 
        random_state=args.randomseed
    ).fit(
        np.array(nons).reshape(-1, 1)
    ) 
    allscores = np.array(scoredf["score"].to_list()).reshape(-1, 1)
    preds = gm.predict_proba(allscores)
    scoredf[["gmm_density_abnormal", "gmm_density_normal"]] = preds
    scoredf["functional_consequence"] = "indeterminate"
    scoredf.loc[scoredf["gmm_density_abnormal"] >= args.gmmthresh, "functional_consequence"] = "functionally_abnormal"
    scoredf.loc[scoredf["gmm_density_normal"] >= args.gmmthresh, "functional_consequence"] = "functionally_normal"

    # avoid the counter-intuitive situation where some high-scoring variants could get abnormal labels owing
    # to heavy tails in the abnormal distribution
    normalmax = scoredf[(scoredf["functional_consequence"] == "functionally_normal") &
                        (scoredf["variant_qc_flag"] == "PASS")]["score"].max()
    scoredf.loc[scoredf["score"] >= normalmax, "functional_consequence"] = "functionally_normal"
    
    return scoredf


def saveCounts(args, df):
    df = df.rename(columns={'allele': 'alt'})
    countcols = [c for c in df.columns if c.startswith("D") and c[-2:] in ("R1", "R2", "R3")]
    outcols = ["chrom", "pos", "ref", "alt", "target", "snvlib_count"]
    outcols.extend(countcols)
    df = df[outcols]
    outfile = args.outputcounts
    df.to_csv(outfile, index=False, sep="\t")
    if args.verbose:
        sys.stderr.write("INFO: wrote counts to %s\n" % outfile)
    return

def saveSNVScores(args, df):
    outfile = args.outfile
    df.to_csv(outfile, index=False, sep="\t", float_format='%g')
    if args.verbose:
        sys.stderr.write("INFO: wrote SNV scores to %s\n" % outfile)
    return
                  

def getAnnots(args, vepdir, targetlist, targetfile):
    '''read the annotations from the VEP output for each tile, and return a dataframe
    '''
    # create a set to hold protein positions where we want to apply a warning
    warning_pos = set()
    if args.verbose:
        sys.stderr.write("INFO: Loading annotation files for variants from %s\n" % vepdir)
    annotdf = pd.DataFrame()
    for target in targetlist:
        vepfile = os.path.join(vepdir, "%s.snvs.vep.tsv" % target)
        vepdf = sge_util.getVEPdf(vepfile)
        vepdf["amino_acid_change"] = vepdf.apply(sge_util.makeAAsub, axis=1)
        vepdf[["chrom", "pos"]] = vepdf["Location"].str.split(":", expand=True)
        vepdf["pos_id"] = vepdf["pos"] + ":" + vepdf["allele"]
        t = sge_target.Target(target, targetfile)
        for pos in t.required_edits:
            protein_pos = vepdf[vepdf["pos"] == str(pos)]["Protein_position"].values[0]
            if protein_pos != '-':
                warning_pos.add(str(protein_pos))
                if args.verbose:
                    sys.stderr.write("INFO: identified required edit at protein position %d\n" % int(protein_pos))
        for pos in t.skip_pos:
            protein_pos = vepdf[vepdf["pos"] == str(pos)]["Protein_position"].values[0]
            if protein_pos != '-':
                warning_pos.add(str(protein_pos))
                if args.verbose:
                    sys.stderr.write("INFO: identified skipped position (cell line SNV?) at protein position %d\n"
                                     % int(protein_pos))
        annotdf = pd.concat(
            [
                annotdf,
                vepdf[
                    ["chrom", "pos", "allele",
                     "pos_id", "amino_acid_change",
                     "Consequence", "hgvs_p",
                     "CDS_position", "cDNA_position",
                     "Protein_position"]
                ]
            ]
        )
    annotdf["warn_codon"] = ""
    annotdf.loc[annotdf["Protein_position"].astype(str).isin(warning_pos), "warn_codon"] = "WARN"
    annotdf = annotdf.drop_duplicates(subset=["pos_id"])
    return annotdf


def runModel(row, days):
    '''apply the simple scoring model to each row in the dataframe, 
    return the score and standard error of the score
    '''
    x = []
    y = []
    colnames = list(row.index)

    for d in days:
        cols = [c for c in colnames if c.startswith(d) and c.endswith("_log2_adjusted")]
        for c in cols:
            if not pd.isna(row[c]):
                x.append(int(d[1:]))
                y.append(row[c])
    x.append(0)
    y.append(0)
    x=np.array(x)#.reshape(-1, 1)
    y=np.array(y)#.reshape(-1, 1)
    
    try:
        model = linregress(x, y)
    except:
        return [None, None]
    
    return [model.slope, model.stderr]


def runModelDups(row, days):
    '''for variants that overlap in two adjacent tiles, run the model
    using data from both tiles to produce a unified score. return the
    score and the standard error of the score
    '''
    x = []
    y = []
    colnames = list(row.index)

    for d in days:
        cols = [c for c in colnames if c.startswith(d) and c.endswith("_log2_adjusted")]
        for c in cols:
            for myvalue in row[c]: # iterate thru list
                if not pd.isna(myvalue):
                    x.append(int(d[1:]))
                    y.append(myvalue)
    x.append(0)
    y.append(0)
    x=np.array(x)#.reshape(-1, 1)
    y=np.array(y)#.reshape(-1, 1)
    try:
        model = linregress(x, y)
    except:
        return [None, None]
    return [model.slope, model.stderr]


def handleDups(args, countsdf, dupdf):
    '''handle the duplicated, overlapping variants in adjacent tiles
    '''
    if args.verbose:
        sys.stderr.write("INFO: Collapsing scores of duplicated variants\n")
    filtdf = countsdf[countsdf["pos_id"].isin(dupdf["pos_id"])]
    if args.verbose:
        sys.stderr.write("INFO: identified %d duplicate entries\n" % filtdf.shape[0])
    tmpdf = filtdf.groupby(["pos_id"]).agg(list).reset_index()
    tmpdf[['dupscore', 'dupse']] = tmpdf.apply(
        runModelDups,
        axis=1,
        result_type='expand',
        days=args.daystrings
    )
    return tmpdf[["pos_id", "dupscore", "dupse"]]


def scoreTarget(args, tdf, t):
    if args.verbose:
        sys.stderr.write("INFO: Scoring variants in %s\n" % t)

    days = args.daystrings
    tdf[['modelscore', 'modelse']] = tdf.apply(
        runModel,
        axis=1,
        result_type='expand',
        days=days
    )
    resdf = tdf[[
        "target", "exon", "chrom", "pos", "allele", "ref", "pos_id", "modelscore", "modelse"
    ]]
    return resdf



# use a loess smoother to smooth the non-day5 frequencies
def smoothe_freqs(args, df, daycounts):
    filtercols = [col for col in df.columns if col.startswith("D05_") \
                  and col.endswith("over_snvlib")]
    cond = (df[filtercols[0]] >= 0.5)
    if len(filtercols) > 1:
        for col in filtercols[1:]:
            cond &= (df[col] >= 0.5)

    fullcount = df.shape[0]
    filtdf = df[cond]
    filtcount = filtdf.shape[0]

    allcoords = df["pos"].unique()
    if args.verbose:
        sys.stderr.write("INFO: smoothing started with %d variants\n" % fullcount)
        sys.stderr.write("INFO: after filtering with threshold=%f, %d variants remain\n" % (0.5,
                                                                           filtcount))

    for col in filtercols:
        loesscol = col.replace("over_snvlib", "loess_adjustment")
        xlist = filtdf['pos'].to_numpy()
        ylist = filtdf[col].to_numpy()
        ylist = np.log2(ylist)
        xout, yout, wout = loess.loess_1d.loess_1d(
            xlist,
            ylist,
            xnew=allcoords,
            frac=0.10
        )
        mydata = np.array([xout, yout])
        tmpdf = pd.DataFrame(
            data=mydata.T, 
            columns=["pos", loesscol]
        ).drop_duplicates(
            subset=['pos'], 
            keep='first'
        )
        tmpdf['pos'] = tmpdf['pos'].astype(int)

        df = df.merge(tmpdf, on="pos")

        for d in daycounts:
            origcol = col.replace("D05", d) + "_log2"
            newcol = origcol + "_adjusted"
            df[newcol] = df[origcol] - df[loesscol]

    return df


def filterCounts(args, tdf, t, all_countcols, all_freqcols):
    if args.verbose:
        sys.stderr.write("INFO: filtering counts for target %s\n" % t)
        sys.stderr.write("INFO: before filtering, dataframe has %d variants\n" % tdf.shape[0])


    freqcols = [col for col in all_freqcols if col[:3] in args.filterstrings] # and col.endswith("freq")]
    countcols = [col for col in all_countcols if col[:3] in args.filterstrings]# and not col.endswith("freq")]

    minfreq = args.minfreq
    mincount = args.mincount
    cnd = (tdf["target"] == t)
    for fcol in freqcols:
        cnd &= (tdf[fcol] >= minfreq)
    for ccol in countcols:
        cnd &= (tdf[ccol] >= mincount)
    tdf = tdf[cnd]

    if args.verbose:
        sys.stderr.write("INFO: after filtering, dataframe has %d variants\n" % tdf.shape[0])
    return tdf


def loadCounts(args, targetfile, t):
    if args.verbose:
        sys.stderr.write("INFO: working on target %s\n" % t)

    target = sge_target.Target(t, targetfile)
    countcols = []
    freqcols = []
    # load the day 0 library
    snvfiles = target.getSNVSampleList(args.countsdir, include_neg=False)
    libfile = snvfiles['D00'][0]
    df = sge_counts.getSNVCounts(
        libfile,
        augment=True,
        pseudocount=1
    )
    df = df.rename(
        columns={'count': 'snvlib_count'}
    ).drop(
        columns=["sampleid", "repl", "day"]
    )
    statsfile = libfile.replace(".snvs.tsv", ".readstats.tsv")
    statsdf = sge_counts.getReadStats(statsfile, augment=False)
    freqcol = "snvlib_freq"
    normval = statsdf["snv_reads"][0]
    df[freqcol] = df["snvlib_count"] / normval

    # load the various timepoints and replicates
    nsamples = 1
    daycounts = {}

    for snvday, snvfilelist in sorted(snvfiles.items()):
        if snvday == "D00": continue  # already loaded
        shortday = int(snvday[1:])
        if shortday not in args.days:
            sys.stderr.write("WARN: Skipping data for day %d as instructed\n" % shortday)
            continue

        for snvfile in sorted(snvfilelist):
            # exclude neg ctrl
            if "_NC_" in snvfile:
                continue
            nsamples += 1
            if snvday in daycounts:            
                daycounts[snvday] += 1
            else:
                daycounts[snvday] = 1

            if args.verbose:
                sys.stderr.write("INFO: loading data from %s\n" % snvfile)
            tmpdf = sge_counts.getSNVCounts(
                snvfile,
                augment=False,
                pseudocount=1
            )
            statsfile = snvfile.replace(".snvs.tsv", ".readstats.tsv")
            statsdf = sge_counts.getReadStats(statsfile, augment=False)
            countcol = "%s_R%d" % (snvday, daycounts[snvday])
            tmpdf = tmpdf.rename(columns={'count': countcol})
            countcols.append(countcol)
            freqcol = countcol + "_freq"
            normval = statsdf["snv_reads"][0]
            tmpdf[freqcol] = tmpdf[countcol] / normval
            freqcols.append(freqcol)
            df = df.merge(
                tmpdf[["chrom", "pos", "allele", countcol,
                       freqcol  
                       ]],
                on=["chrom", "pos", "allele"]
            )
            libratiocol = "%s_over_snvlib" % (countcol)
            df[libratiocol] = df[freqcol] / df["snvlib_freq"]
            log2libratiocol = "%s_over_snvlib_log2" % (countcol)
            df[log2libratiocol] = np.log2(df[libratiocol])

    if args.verbose:
        sys.stderr.write("INFO: Finished loading data from %d samples\n" % nsamples)
        sys.stderr.write("INFO: Using data from timepoints: %s\n" % (', '.join(list(daycounts.keys()))))

    # filter the counts for only the edited bases
    df = df[(df["pos"] >= target.editstartpos) &
            (df["pos"] <= target.editendpos)]

    # merge with the reference sequence
    df = df.merge(target.refdf, on="pos")
    df = df[df["ref"] != df["allele"]]
    df["pos_id"] = df["pos"].astype(str) + ":" + df["allele"]
    df = df[~df["pos"].isin(target.required_edits)]
    df = df[~df["pos"].isin(target.skip_pos)]

    # add target metadata
    df["target"] = t
    if t[-1] not in string.digits:
        shorttarget = t[:-1]
    else:
        shorttarget = t
    df["exon"] = shorttarget

    # filter low counts/freqs
    df = filterCounts(
        args,
        df,
        t,
        countcols,
        freqcols
    )

    # smoothe the freqs
    df = smoothe_freqs(
        args,
        df,
        daycounts
    )

    return df


def main():
    parser = argparse.ArgumentParser(
        prog='scoreSNVs',
        description='produce SNV scores from counts for a set of tiles in a gene'
    )
    parser.add_argument(
        '-o', '--outfile', required=True,
        help="Path to output TSV file of scores"
    )
    parser.add_argument(
        '-C', '--outputcounts', required=False, default="",
        help="Path to output TSV file of per-tile counts for each replicate"
    )
    parser.add_argument(
        '-g', '--gene', required=True,
        help='Gene to score'
    )
    parser.add_argument(
        '-V', '--vepdir', required=False,
        help="Path to directory with VEP files"
    )
    parser.add_argument(
        '-e', '--ensemblfile', required=False,
        default="/net/bbi/vol1/data/sge-analysis/etc/extended_ensembl_consequence.tsv",
        help="Path to ensembl consequence groupings in TSV format"
    )
    parser.add_argument(
        '-t', '--targetfile', required=False,
        help='Path to target file (can be auto detected)'
    )
    parser.add_argument(
        '-v', '--verbose', required=False, default=False,
        action="store_true", help="Verbose output"
    )
    parser.add_argument(
        '-c', '--countsdir', required=True,
        help='Load counts files from <countsdir>'
    )
    parser.add_argument(
        '-x', '--exclude', required=False,
        help='Exclude these regions when scoring a gene (comma-sep list)'
    )
    parser.add_argument(
        '-X', '--nononsense', required=False,
        help="Exclude nonsense variants in these targets when assigning functional classes"
    )
    parser.add_argument(
        '-m', '--mincount', required=False, type=int, default=0,
        help='filter out variants with raw counts less than <mincount> in any replicate'
    )
    parser.add_argument(
        '-M', '--minfreq', required=False, type=float, default=0.,
        help='filter out variants with frequencies less than <minfreq> in any replicate'
    )
    parser.add_argument(
        '-f', '--filter', required=False, type=str,
        help="Comma-separated list of days to use in count filtering (e.g., -f 5 or -f 5,13,17 )"
    )
    parser.add_argument(
        '-d', '--days', required=True, type=str,
        help="Comma-separated list of days to use in model (e.g., -d 5,13 or -d 5,13,17 )"
    )
    parser.add_argument(
        '-r', '--randomseed', required=False, default=54321, type=int,
        help="Random seed (integer) for GMM estimation algorithm (default 54321)"
    )
    parser.add_argument(
        '-T', '--gmmthresh', required=False, default=0.95, type=float,
        help="Minimum estimated GMM component density to assign functional class (def. 0.95)"
    )
    parser.add_argument(
        '-F', '--minfunc', required=False, default=1000, type=int,
        help="Minimum number of scored variants required before functional class assignment"
    )
                        
    args = parser.parse_args()

    # handle the targetfile

    if not args.targetfile:
        targetfile = sge_util.guess_target_file(args.gene)
    else:
        targetfile = args.targetfile
    if not targetfile or not os.path.exists(targetfile):
        sys.stderr.write("ERROR: Can't locate target file\n")
        sys.exit(-99)

    # handle the VEP output directory
    if not args.vepdir:
        vepdir = "/net/bbi/vol1/data/sge-analysis/etc/%s/" % args.gene
    else:
        vepdir = args.vepdir
    if not os.path.isdir(vepdir):
        sys.stderr.write("ERROR: Can't find VEP directory %s\n" % vepdir)
        sys.exit(-99)

    if not args.days:
        sys.stderr.write("ERROR: Must supply -d / --days\n")
        sys.exit(-99)

    try:
        args.days = [int(s.strip()) for s in args.days.split(",")]
        # turn [5, 13, 17] into ["D05", "D13", "D17"]
        args.daystrings = ["D" + str(d).zfill(2) for d in args.days]
        sys.stderr.write("INFO: using days %s in model\n" % ', '.join(map(str, args.days)))

    except:
        sys.stderr.write("ERROR: Unable to parse days from -d / --days argument: %s\n" % args.days)
        sys.exit(-99)

    # parse the filtering days
    if not args.filter:
        sys.stderr.write("INFO: No count-based filtering will be applied\n")
        args.filter = []
        args.filterstrings = []

    else:
        try:
            args.filter = [int(s.strip()) for s in args.filter.split(",")]
            # turn [5, 13, 17] into ["D05", "D13", "D17"]
            args.filterstrings = ["D" + str(d).zfill(2) for d in args.filter]

            sys.stderr.write("INFO: will filter based on days %s\n" % ', '.join(map(str, args.filter)))

        except:
            sys.stderr.write("ERROR: Unable to parse days from -f / --filter argument: %s\n" % args.filter)
            sys.exit(-99)

    # load the target file and exclude any targets if necessary
    targetdf = pd.read_csv(targetfile, sep="\t", header=0)
    targetlist = targetdf["target"].to_list()

    if args.verbose:
        sys.stderr.write(
            "INFO: Found %d targets in targetfile %s\n" % (
                len(targetlist),
                targetfile
            )
        )

    # check for excluded targets
    exclude_targets = []
    if args.exclude:
        parts = args.exclude.split(",")
        for p in parts:
            exclude_targets.append(p)
    targetlist = [tl for tl in targetlist if tl not in exclude_targets]
    if args.verbose:
        sys.stderr.write("INFO: After processing exclusions, %d targets remain\n" % (len(targetlist)))

    # check for targets to exclude from nonsense z score percentile calculation
    exclude_nonsense = []
    exclude_nonsense_coords = set()
    if args.nononsense:
        parts = args.nononsense.split(",")
        for p in parts:
            exclude_nonsense.append(p)
    for target in exclude_nonsense:
        edit_start_pos = targetdf.loc[targetdf["target"] == target, "editstart"].values[0]
        edit_stop_pos = targetdf.loc[targetdf["target"] == target, "editstop"].values[0]
        for pos in range(edit_start_pos, edit_stop_pos + 1, 1):
            exclude_nonsense_coords.add(pos)
    if args.verbose:
        sys.stderr.write("INFO: Will exclude %d positions from nonsense threshold computation\n" % len(exclude_nonsense_coords))

    annotdf = getAnnots(args, vepdir, targetlist, targetfile)
    countsdf = pd.DataFrame()
    scoredf = pd.DataFrame()
    for t in targetlist:
        try:
            tdf = loadCounts(args, targetfile, t)
        except:
            sys.stderr.write("WARN: couldn't load counts for target %s, skipping\n" % t)
            continue
        countsdf = pd.concat([countsdf, tdf])
        tscoresdf = scoreTarget(args, tdf, t)
        scoredf = pd.concat([scoredf, tscoresdf])

    # now done with individual targets, so we need to collapse dups
    dupdf = scoredf[scoredf.duplicated("pos_id", keep=False) == True]
    dupscoredf = handleDups(args, countsdf, dupdf)    

    scoredf = scoredf.merge(
        dupscoredf[["pos_id", "dupscore", "dupse"]],
        on="pos_id", 
        how="outer"
    )
    scoredf = scoredf.drop_duplicates(subset=["pos_id"])
    scoredf["score"] = scoredf["dupscore"]
    scoredf["standard_error"] = scoredf["dupse"]
    scoredf.loc[scoredf["score"].isna(), "score"] = scoredf["modelscore"]
    scoredf.loc[scoredf["standard_error"].isna(), "standard_error"] = scoredf["modelse"]

    scoredf["95_ci_upper"] = scoredf["score"] + (1.96 * scoredf["standard_error"])
    scoredf["95_ci_lower"] = scoredf["score"] - (1.96 * scoredf["standard_error"])
    scoredf = scoredf.merge(
        annotdf[["pos_id", "Consequence",
                 "amino_acid_change", "hgvs_p",
                 "CDS_position", "cDNA_position",
                 "warn_codon"]],
        on="pos_id"
    )

    scoredf["simplified_consequence"] = scoredf["Consequence"].apply(
        get_simplified_consequence,
        ensemblfile=args.ensemblfile
    )
    scoredf = scoredf.drop(
        columns=["Consequence"]
    ).rename(
        columns={'simplified_consequence': 'consequence',
                 'allele': 'alt'}
    )
    
    scoredf["variant_qc_flag"] = "PASS"
    scoredf.loc[scoredf["warn_codon"] == "WARN", "variant_qc_flag"] = "WARN"
    if scoredf.shape[0] >= args.minfunc:
        if args.verbose:
            sys.stderr.write(
                "INFO: Dataset has at least %d variants (%d); proceeding with functional class assignment\n" % (
                    args.minfunc,
                    scoredf.shape[0]
                )
            )
        scoredf = makeFunctionalClasses(
            args,
            scoredf,
            exclude_nonsense_coords
        )
    else:
        if args.verbose:
            sys.stderr.write(
                "INFO: Dataset has less than %d variants (%d); not performing functional class assignment\n" % (
                    args.minfunc,
                    scoredf.shape[0]
                )
            )
        scoredf["functional_consequence"] = ""
        scoredf["gmm_density_abnormal"] = ""
        scoredf["gmm_density_normal"] = ""
               
    if args.outputcounts:
        saveCounts(args, countsdf)

    scoredf = scoredf[[
        "chrom", "pos", "ref", "alt", "exon", "target",
        "consequence", "score", 
        "standard_error", "95_ci_upper", "95_ci_lower",
        "amino_acid_change", "hgvs_p", "CDS_position", "cDNA_position", 
        "functional_consequence",
        "gmm_density_abnormal", "gmm_density_normal",
        "variant_qc_flag",
    ]]

    saveSNVScores(args, scoredf)
    return 0
    


if __name__ == '__main__':
    main()
