#!/usr/bin/env python

import sys
import argparse
import pandas as pd
import os
import string

from pydeseq2.dds import DeseqDataSet
from pydeseq2.default_inference import DefaultInference
from pydeseq2.ds import DeseqStats

sys.path.append("/net/bbi/vol1/data/sge-analysis/lib/")
import sge_util
import sge_counts
import sge_target

from scipy.stats import norm

# Function to assign the most important SO summary term
def get_simplified_consequence(vep_terms):
    consequence_df = pd.read_csv("/net/bbi/vol1/data/sge-analysis/etc/extended_ensembl_consequence.tsv", sep='\t')
    # Create a dictionary mapping terms to their SO summary term (prioritizing importance)
    mapping_dict = (
        consequence_df.sort_values(by="Importance", ascending=False)
        .groupby("VEP output term")[["SO summary term", "Importance"]]
        .first()
        .to_dict()["SO summary term"]
    )
    if not isinstance(vep_terms, str):  
        return "Unknown"
    terms = vep_terms.split(',')  
    matched_terms = [mapping_dict.get(term.strip()) for term in terms if term.strip() in mapping_dict]  # Match terms
    
    return matched_terms[0] if matched_terms else "Unknown"


def saveSNVScores(args, df):
    outfile = args.outfile
    df.to_csv(outfile, index=False, sep="\t", float_format='%g')
    if args.verbose:
        sys.stderr.write("INFO: wrote SNV scores to %s\n" % outfile)
    return


def saveZDistAndThresholds(args, mu, std, lthresh, uthresh):
    filepath = args.savez
    with open(filepath, "w") as outfile:
        outfile.write("#mu_neut\tstd_neut\tlthresh\tuthresh\n")
        outfile.write("%f\t%f\t%f\t%f\n" % (mu, std, lthresh, uthresh))
    if args.verbose:
        sys.stderr.write("INFO: wrote z score and threshold parameters to %s\n" % filepath)
    return
                      

def makeFunctionalClasses(args, scoredf, exclude_nonsense_coords):
    neutral_cnd = ((scoredf["simplified_consequence"].isin(["synonymous_variant", "intron_variant"])) &
                   (scoredf["variant_qc_flag"] == "PASS") &
                   (~scoredf["pos"].isin(exclude_nonsense_coords)))
    nonsense_cnd = ((scoredf["simplified_consequence"].isin(["stop_gained",])) &
                    (scoredf["variant_qc_flag"] == "PASS") &
                    (~scoredf["pos"].isin(exclude_nonsense_coords)))
    nonsense_cnd_nofilt = ((scoredf["simplified_consequence"].isin(["stop_gained",])) &
                           (scoredf["variant_qc_flag"] == "PASS"))
    nonsense_count = scoredf[nonsense_cnd].shape[0]
    nonsense_count_nofilt = scoredf[nonsense_cnd_nofilt].shape[0]
    if args.verbose:
        sys.stderr.write("INFO: identified %d nonsense variants for thresholding\n" %
                         nonsense_count)
        sys.stderr.write("INFO: without exon filtering, %d nonsense variants would have been used\n" %
                         nonsense_count_nofilt)
    
    neutralvars = scoredf[neutral_cnd]["score"].to_list()
    
    mu_neut, std_neut = norm.fit(neutralvars)
    scoredf['functional_consequence_zscore'] = (scoredf['score'] - mu_neut) / std_neut

    # determine the thresholds
    thresholds = []
    thresholds.append(scoredf[neutral_cnd]['functional_consequence_zscore'].quantile(0.01))
    thresholds.append(scoredf[nonsense_cnd]['functional_consequence_zscore'].quantile(0.95))
    lthresh, uthresh = min(thresholds), max(thresholds)
    if args.verbose:
        sys.stderr.write("INFO: identified threshold values of %f and %f\n" % (lthresh, uthresh))
    if args.savez:
        saveZDistAndThresholds(args, mu_neut, std_neut, lthresh, uthresh)
    # apply the labels
    scoredf["functional_consequence"] = "indeterminate"
    scoredf.loc[scoredf["functional_consequence_zscore"] <= lthresh, "functional_consequence"] = "functionally_abnormal"
    scoredf.loc[scoredf["functional_consequence_zscore"] >= uthresh, "functional_consequence"] = "functionally_normal"
    return scoredf


def getAnnots(args, vepdir, targetlist):
    if args.verbose:
        sys.stderr.write("INFO: Loading annotation files for variants from %s\n" % vepdir)
    annotdf = pd.DataFrame()
    for target in targetlist:
        vepfile = os.path.join(vepdir, "%s.snvs.vep.tsv" % target)
        vepdf = sge_util.getVEPdf(vepfile)
        vepdf["amino_acid_change"] = vepdf.apply(sge_util.makeAAsub, axis=1)
        vepdf[["chrom", "pos"]] = vepdf["Location"].str.split(":", expand=True)
        vepdf["pos_id"] = vepdf["pos"] + ":" + vepdf["allele"]
        annotdf = pd.concat([annotdf, vepdf[["chrom", "pos", "allele", "pos_id", "amino_acid_change", "Consequence"]]])
    annotdf = annotdf.drop_duplicates()
    return annotdf


def handleDups(args, countsdf, dupdf):
    if args.verbose:
        sys.stderr.write("INFO: Collapsing scores of duplicated variants\n")
    filtdf = countsdf[countsdf["pos_id"].isin(dupdf["pos_id"])]
    if args.verbose:
        sys.stderr.write("INFO: identified %d duplicate entries\n" % filtdf.shape[0])
    groupdf = filtdf.groupby(["pos_id"]).agg(
        {
            'snvlib_count': ['first', 'last'],
            'D05_R1': ['first', 'last'],
            'D05_R2': ['first', 'last'],
            'D05_R3': ['first', 'last'],
            'D13_R1': ['first', 'last'],
            'D13_R2': ['first', 'last'],
            'D13_R3': ['first', 'last'],
        }
    ).reset_index()
    groupdf.columns = [
        "pos_id", "snvlib_count_A", "snvlib_count_B",
        "D05_R1_A", "D05_R1_B",
        "D05_R2_A", "D05_R2_B",
        "D05_R3_A", "D05_R3_B",
        "D13_R1_A", "D13_R1_B",
        "D13_R2_A", "D13_R2_B",
        "D13_R3_A", "D13_R3_B",
    ]
    
    groupdfmelt = pd.melt(groupdf, id_vars=['pos_id'])
    groupdfpivot = pd.pivot(groupdfmelt, index='variable', columns=['pos_id'], values='value')
    groupdfpivot = groupdfpivot.rename_axis(None, axis=1)
    groupdfpivot = groupdfpivot.rename_axis(None, axis=0)

    metanames = [
        'D05_R1_A', 'D05_R1_B',
        'D05_R2_A', 'D05_R2_B',
        'D05_R3_A', 'D05_R3_B',
        'D13_R1_A', 'D13_R1_B',
        'D13_R2_A', 'D13_R2_B',
        'D13_R3_A', 'D13_R3_B', 
        'snvlib_count_A', 'snvlib_count_B'
    ]

    metadays = [
        5, 5, 5, 5, 5, 5, 
        13, 13, 13, 13, 13, 13, 
        0, 0
    ]
        
    metagroups = [
        "A", "B", "A", "B", "A", "B",
        "A", "B", "A", "B", "A", "B",
        "A", "B"
    ]
        
    metadf = pd.DataFrame(
        {
            'sample_name': metanames,
            'time': metadays,
            'group': metagroups
        }
    )
    metadf = metadf.set_index('sample_name').rename_axis(None, axis=0)
    # fit a continuous time model to estimate per-day LFC
    inference = DefaultInference(n_cpus=1)
    dds = DeseqDataSet(
        counts=groupdfpivot,
        metadata=metadf,
        design="~time+group",
        refit_cooks=True,
        inference=inference,
    )
    dds.deseq2()
    contrast = ["time", 1, 0]
    stat_res = DeseqStats(dds, contrast=contrast, inference=inference)
    stat_res.summary()
    resdf = stat_res.results_df
    resdf = resdf.reset_index(names='pos_id')#.drop(columns=['index'])

    resdf = resdf.rename(
        columns={
            'log2FoldChange': 'overlap_LFC',
            'lfcSE': 'overlap_lfcSE'
            }
    )
    return resdf

    
def scoreTarget(args, tdf, t, nsamples, target_to_scoreday):
    if args.verbose:
        sys.stderr.write("INFO: Scoring variants in %s\n" % t)
    if t[-1] not in string.digits:
        shorttarget = t[:-1]
    else:
        shorttarget = t

    scoreday = target_to_scoreday[t]
    if args.verbose:
        sys.stderr.write("INFO: set score day to %d for target %s\n" % (scoreday, t))

    # construct the matrix for model input
    value_cols = [col for col in tdf.columns if "D13" in col or "D05" in col or "snvlib_count" in col]
    dfmelt = pd.melt(tdf, id_vars=['pos_id'], value_vars=value_cols)
    dfpivot = pd.pivot(dfmelt, index='variable', columns=['pos_id'], values='value')
    dfpivot = dfpivot.rename_axis(None, axis=1)
    dfpivot = dfpivot.rename_axis(None, axis=0)

    # construct the metadata
    if nsamples == 7:
        metanames = ['D05_R1', 'D05_R2', 'D05_R3', 'D13_R1', 'D13_R2', 'D13_R3', 'snvlib_count']
        metadays = [5, 5, 5, scoreday, scoreday, scoreday, 0]

    elif nsamples == 5:
        metanames = ['D05_R1', 'D05_R2', 'D13_R1', 'D13_R2', 'snvlib_count']
        metadays = [5, 5, scoreday, scoreday, 0]
        
    metadf = pd.DataFrame(
        {'sample_name': metanames,
        'time': metadays,
        }
    )
    metadf = metadf.set_index('sample_name').rename_axis(None, axis=0)

    # fit a continuous time model to estimate per-day LFC
    inference = DefaultInference(n_cpus=1)  # no good reason to increase n_cpus for this scale of data
    dds = DeseqDataSet(
        counts=dfpivot,
        metadata=metadf,
        design="~time",
        refit_cooks=True,
        inference=inference,
    )
    dds.deseq2()
    contrast = ["time", 1, 0]
    stat_res = DeseqStats(dds, contrast=contrast, inference=inference)
    stat_res.summary()
    resdf = stat_res.results_df
    resdf = resdf.reset_index(names='pos_id')#.drop(columns=['index'])
    resdf = resdf.merge(tdf[[
        "chrom", "pos", "allele", "ref", "pos_id", 
    ]])
    resdf["target"] = t
    resdf["exon"] = shorttarget
    return resdf


def filterCounts(args, tdf, t):
    if args.verbose:
        sys.stderr.write("INFO: filtering counts for target %s\n" % t)
        sys.stderr.write("INFO: before filtering, dataframe has %d variants\n" % tdf.shape[0])
    
    freqcols = [col for col in tdf.columns if ("D13" in col or "D05" in col or "snvlib" in col) 
                and "freq" in col]
    countcols = [col for col in tdf.columns if ("D13" in col or "D05" in col or "snvlib_count" in col) 
                and "freq" not in col]
    minfreq = args.minfreq
    mincount = args.mincount
    cnd = (tdf["target"] == t)
    for fcol in freqcols:
        cnd &= (tdf[fcol] >= minfreq)
    for ccol in countcols:
        cnd &= (tdf[ccol] >= mincount)
    tdf = tdf[cnd]
    # we don't need freqcols anymore
    tdf = tdf.drop(columns=freqcols)
    if args.verbose:
        sys.stderr.write("INFO: after filtering, dataframe has %d variants\n" % tdf.shape[0])
    return tdf


def loadCounts(args, targetfile, t):
    if args.verbose:
        sys.stderr.write("INFO: working on target %s\n" % t)

    target_to_scoreday = {}
    target = sge_target.Target(t, targetfile)
    # load the day 0 library
    snvfiles = target.getSNVSampleList(args.countsdir,
        include_neg=False)
    libfile = snvfiles['D00'][0]
    df = sge_counts.getSNVCounts(libfile,
                                augment=True,
                                pseudocount=1)
    df = df.rename(columns={'count': 'snvlib_count'}
            ).drop(columns=["sampleid", "repl", "day"])

    # load the various timepoints and replicates
    nsamples = 1
    allrepls = {}
    for snvday, snvfilelist in sorted(snvfiles.items()):
        if snvday == "D00": continue  # already loaded
        allrepls[snvday] = []
        for snvfile in sorted(snvfilelist):
            # exclude neg
            if "_NC_" in snvfile:
                continue
            nsamples += 1
            if args.verbose:
                sys.stderr.write("INFO: loading data from %s\n" % snvfile)
            tmpdf = sge_counts.getSNVCounts(snvfile,
                                            augment=False,
                                            pseudocount=1)
            statsfile = snvfile.replace(".snvs.tsv", ".readstats.tsv")
            statsdf = sge_counts.getReadStats(statsfile, augment=False)
            sampleid = tmpdf["sampleid"][0]
            parts = sampleid.split("_")
            repl = parts[2]
            allrepls[snvday].append(repl)
            countcol = "%s_%s" % (snvday, repl)
            tmpdf = tmpdf.rename(columns={'count': countcol})
            
            # comment out this block
            freqcol = countcol + "_freq"
            normval = statsdf["snv_reads"][0]
            tmpdf[freqcol] = tmpdf[countcol] / normval
            
            df = df.merge(tmpdf[["chrom", "pos", "allele", countcol, 
                                freqcol  # comment this out
                                ]], on=["chrom", "pos", "allele"])
    
    if args.verbose:
        sys.stderr.write("INFO: Finished loading data from %d samples\n" % nsamples)
        sys.stderr.write("INFO: Found data from timepoints: %s\n" % (', '.join(list(allrepls.keys()))))

    if "D12" in allrepls.keys():
        target_to_scoreday[t] = 12
    else:
        target_to_scoreday[t] = 13

    # filter the counts for only the edited bases
    df = df[(df["pos"] >= target.editstartpos) &
            (df["pos"] <= target.editendpos)]
    
    # merge with the reference sequence
    df = df.merge(target.refdf, on="pos")
    df = df[df["ref"] != df["allele"]]
    df["pos_id"] = df["pos"].astype(str) + ":" + df["allele"]
    df = df[~df["pos"].isin(target.required_edits)]
    df = df[~df["pos"].isin(target.skip_pos)]

    # this next bit needs to be more flexible if the days are not day 5/13

    # now we need to convert this to n_samples as rows, and n_variants as columns
    value_cols = [col for col in df.columns if "D13" in col or "D12" in col or "D05" in col or "snvlib_count" in col
                ]
    d5_count = 1
    d13_count = 1
    for colname in df.columns:
        if colname.startswith("D05"):
            if not colname.endswith("_freq"):
                df = df.rename(columns={colname: "D05_R%d" % d5_count})
            else:
                df = df.rename(columns={colname: "D05_R%d_freq" % d5_count})
                d5_count += 1

        if colname.startswith("D13") or colname.startswith("D12"):
            if not colname.endswith("_freq"):
                df = df.rename(columns={colname: "D13_R%d" % d13_count})
            else:
                df = df.rename(columns={colname: "D13_R%d_freq" % d13_count})
                d13_count += 1

    #sys.stderr.write("INFO: updated colnames: %s\n" % (', '.join(value_cols)))
    df["target"] = t
    return df, nsamples, target_to_scoreday


def main():
    parser = argparse.ArgumentParser('combine and normalize SNV scores across a gene')
    parser.add_argument('-o', '--outfile', required=True,
                        help="Path to output TSV file of scores")
    parser.add_argument('-z', '--savez', required=False, default="",
                        help="Path to output TSV file of Z dist parameters and thresholds")
    parser.add_argument('-g', '--gene', required=True,
                        help='Gene to score')
    parser.add_argument('-V', '--vepdir', required=False,
                        help="Path to directory with VEP files")
    parser.add_argument('-t', '--targetfile', required=False,
                        help='Path to target file (can be auto detected)')
    parser.add_argument('-v', '--verbose', required=False, default=False,
                        action="store_true", help="Verbose output")
    parser.add_argument('-c', '--countsdir', required=True,
                        help='Load counts files from <countsdir>')
    parser.add_argument('-x', '--exclude', required=False,
                        help='Exclude these regions when scoring a gene (comma-sep list)')
    parser.add_argument('-X', '--nononsense', required=False,
                         help="Exclude nonsense variants in these targets when assigning functional classes")
    parser.add_argument('-m', '--mincount', required=False, type=int, default=0,
                        help='filter out variants with raw counts less than <mincount> in any replicate')
    parser.add_argument('-M', '--minfreq', required=False, type=float, default=0.,
                        help='filter out variants with frequencies less than <minfreq> in any replicate')

    args = parser.parse_args()

    # handle the targetfile
    if not args.targetfile:
        targetfile = sge_util.guess_target_file(args.gene)
    else:
        targetfile = args.targetfile
    if not targetfile or not os.path.exists(targetfile):
        sys.stderr.write("ERROR: Can't locate target file\n")
        sys.exit(-99)
    
    # handle the VEP output directory
    if not args.vepdir:
        vepdir = "/net/bbi/vol1/data/sge-analysis/etc/%s/" % args.gene
    else:
        vepdir = args.vepdir
    if not os.path.isdir(vepdir):
        sys.stderr.write("ERROR: Can't find VEP directory %s\n" % vepdir)
        sys.exit(-99)

    # load the target file and exclude any targets if necessary
    targetdf = pd.read_csv(targetfile, sep="\t", header=0)
    targetlist = targetdf["target"].to_list()
    
    if args.verbose:
        sys.stderr.write("INFO: Found %d targets in targetfile %s\n" % (len(targetlist),
                                                                        targetfile))

    # check for excluded targets
    exclude_targets = []
    if args.exclude:
        parts = args.exclude.split(",")
        for p in parts:
            exclude_targets.append(p)
    targetlist = [tl for tl in targetlist if tl not in exclude_targets]
    if args.verbose:
        sys.stderr.write("INFO: After processing exclusions, %d targets remain\n" % (len(targetlist)))

    # check for targets to exclude from nonsense z score percentile calculation
    exclude_nonsense = []
    exclude_nonsense_coords = set()
    if args.nononsense:
        parts = args.nononsense.split(",")
        for p in parts:
            exclude_nonsense.append(p)
    for target in exclude_nonsense:
        edit_start_pos = targetdf.loc[targetdf["target"] == target, "editstart"].values[0]
        edit_stop_pos = targetdf.loc[targetdf["target"] == target, "editstop"].values[0]
        for pos in range(edit_start_pos, edit_stop_pos + 1, 1):
            exclude_nonsense_coords.add(pos)
    if args.verbose:
        sys.stderr.write("INFO: Will exclude %d positions from nonsense threshold computation\n" % len(exclude_nonsense_coords))

    annotdf = getAnnots(args, vepdir, targetlist)
    countsdf = pd.DataFrame()
    scoredf = pd.DataFrame()
    for t in targetlist:
        tdf, nsamples, target_to_scoreday = loadCounts(args, targetfile, t)
        tdf = filterCounts(args, tdf, t)
        countsdf = pd.concat([countsdf, tdf])
        tscoresdf = scoreTarget(args, tdf, t, nsamples, target_to_scoreday)
        scoredf = pd.concat([scoredf, tscoresdf])

    # at this point we've scored all the targets.  now we need to identify the 
    # variants that are covered twice

    dupdf = scoredf[scoredf.duplicated("pos_id", keep=False) == True]
    dupscoredf = handleDups(args, countsdf, dupdf)
    scoredf = scoredf.merge(dupscoredf[["pos_id", "overlap_LFC", "overlap_lfcSE"]],
                            on="pos_id", how="outer")
    scoredf = scoredf.drop_duplicates(subset=["pos_id"])
    scoredf["score"] = scoredf["overlap_LFC"]
    scoredf["standard_error"] = scoredf["overlap_lfcSE"]
    scoredf.loc[scoredf["score"].isna(), "score"] = scoredf["log2FoldChange"]
    scoredf.loc[scoredf["standard_error"].isna(), "standard_error"] = scoredf["lfcSE"]
    scoredf["95_ci_upper"] = scoredf["score"] + (1.96 * scoredf["standard_error"])
    scoredf["95_ci_lower"] = scoredf["score"] - (1.96 * scoredf["standard_error"])
    scoredf = scoredf.merge(annotdf[["pos_id", "Consequence",
                                     "amino_acid_change"]], on="pos_id")

    scoredf["simplified_consequence"] = scoredf["Consequence"].apply(get_simplified_consequence)

    scoredf["variant_qc_flag"] = "PASS"
    scoredf = makeFunctionalClasses(args, scoredf, exclude_nonsense_coords)

    # now add counts
    snvlibs = countsdf.groupby(['pos_id'])['snvlib_count'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'snvlib_lib1', 1: 'snvlib_lib2'})
    
    d5r1 = countsdf.groupby(['pos_id'])['D05_R1'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'D05_R1_lib1', 1: 'D05_R1_lib2'})
    d5r2 = countsdf.groupby(['pos_id'])['D05_R2'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'D05_R2_lib1', 1: 'D05_R2_lib2'})
    d5r3 = countsdf.groupby(['pos_id'])['D05_R3'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'D05_R3_lib1', 1: 'D05_R3_lib2'})

    d13r1 = countsdf.groupby(['pos_id'])['D13_R1'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'D13_R1_lib1', 1: 'D13_R1_lib2'})
    d13r2 = countsdf.groupby(['pos_id'])['D13_R2'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'D13_R2_lib1', 1: 'D13_R2_lib2'})
    d13r3 = countsdf.groupby(['pos_id'])['D13_R3'].apply(lambda x: pd.Series(list(x))).unstack().reset_index().rename(columns={0: 'D13_R3_lib1', 1: 'D13_R3_lib2'})
    
    
    scoredf = scoredf.merge(snvlibs, on="pos_id",
                ).merge(d5r1, on="pos_id"
                ).merge(d5r2, on="pos_id"
                ).merge(d5r3, on="pos_id"
                ).merge(d13r1, on="pos_id"
                ).merge(d13r2, on="pos_id"
                ).merge(d13r3, on="pos_id")
    
    scoredf = scoredf[[
        "chrom", "pos", "ref", "allele", "exon", "target",
        "simplified_consequence", "score", "standard_error", "95_ci_upper", "95_ci_lower",
        "amino_acid_change", "functional_consequence",
        "functional_consequence_zscore", "variant_qc_flag",
        "snvlib_lib1", "snvlib_lib2",
        "D05_R1_lib1", "D05_R1_lib2",
        "D05_R2_lib1", "D05_R2_lib2",
        "D05_R3_lib1", "D05_R3_lib2",
        "D13_R1_lib1", "D13_R1_lib2",
        "D13_R2_lib1", "D13_R2_lib2",
        "D13_R3_lib1", "D13_R3_lib2",
    ]]

    saveSNVScores(args, scoredf)
    return 0


if __name__ == '__main__':
    main()
    
