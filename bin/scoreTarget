#!/usr/bin/env python

import sys
import argparse
from collections import defaultdict
import altair as alt
import pandas as pd
import numpy as np
import loess.loess_1d

sys.path.append("/net/bbi/vol1/data/sge-analysis/lib/")
import sge_util
import sge_counts
import sge_altair
import sge_target


def saveDelScoreFigure(target, args, plotdf):
    delfiles = target.getDelSampleList(args.countsdir,
                                       include_neg=False)
    alldays = sorted(delfiles.keys())
    alldays.remove('D00')
    lateday = alldays[1]
    scorecol = "%s_log2_median" % lateday
    overallmax = np.nanmax(plotdf[np.isfinite(plotdf[scorecol])][scorecol])
    overallmin = np.nanmin(plotdf[np.isfinite(plotdf[scorecol])][scorecol])

    a = alt.Chart(
            plotdf, height=300, width=1200
        ).mark_rule(
            size=5, clip=True
        ).encode(
            x=alt.X(
                'start:Q', title='Deletion coordinates'
            ).scale(
                zero=False
            ),
            x2='end:Q',
            y=alt.Y(
                '%s:Q' % scorecol, title='median log2 ratio'
            ).scale(
                zero=False
            ),
            color=alt.Color(
                'Consequence:N', 
                scale=alt.Scale(scheme='category10'),
            ),
            tooltip=['chrom', 'start', 'end','Consequence', scorecol]
        ) 
    
    b = alt.Chart(plotdf, width=550, height=250
            ).mark_bar(
        opacity=0.9,
        binSpacing=0
    ).encode(
        x=alt.X('%s:Q' % scorecol, title="median log2 ratio", 
            axis=alt.Axis(labelFontSize=12, labelAngle=315)).bin(extent=[overallmin, overallmax], step=0.1),
        y=alt.Y('count()', title="number of deletions").stack("zero"),
        color=alt.Color('Consequence:N', title="Variant type", scale=alt.Scale(scheme="category10"))
    )

    c = alt.Chart(plotdf, width=550, height=250
            ).mark_bar(
        opacity=0.9,
        binSpacing=0
    ).encode(
        x=alt.X('%s:Q' % scorecol, title="median log2 ratio", 
            axis=alt.Axis(labelFontSize=12, labelAngle=315)).bin(extent=[overallmin, overallmax], step=0.1),
        y=alt.Y('count()', title="Distribution within score bin").stack("normalize"),
        color=alt.Color('Consequence:N', title="Variant type", scale=alt.Scale(scheme="category10"))
    )
    scorechart = (a & (b | c)).properties(
        title="Deletion scores for %s at timepoint %s" % (target.targetname, lateday)
    ).configure_title(
        fontSize=20, 
        offset=5, 
        orient='top', 
        anchor='middle',
    )
    scorechart.save(args.delfig)
    return


def saveSNVScoreFigure(target, args, plotdf):
    snvfiles = target.getSNVSampleList(args.countsdir,
                                       include_neg=False)
    alldays = sorted(snvfiles.keys())
    alldays.remove('D00')
    lateday = alldays[1]
    #scorecol = "%s_log2_median" % lateday
    scorecol = "target_score"
    overallmax = np.nanmax(plotdf[np.isfinite(plotdf[scorecol])][scorecol])
    overallmin = np.nanmin(plotdf[np.isfinite(plotdf[scorecol])][scorecol])

    a = alt.Chart(plotdf, height=200, width=1200).mark_point(filled=True, size=50).encode(
        x=alt.X('pos:N', title="Position" ,sort='x', axis=alt.Axis(labelFontSize=10)
    ),
        y=alt.Y('%s:Q' % scorecol, title="median log2 ratio", scale=alt.Scale()),
        color=alt.Color('Consequence:N', title="Variant type"),
        #opacity=alt.condition(consequence_selection, alt.value(1), alt.value(0.2)),
        tooltip=['pos', 'allele', 'ref', 'pos_id','Consequence', scorecol]
    ).interactive()

    b = alt.Chart(plotdf, width=550, height=250
            ).mark_bar(
        opacity=0.9,
        binSpacing=0
    ).encode(
        x=alt.X('%s:Q' % scorecol, title="median log2 ratio", 
            axis=alt.Axis(labelFontSize=12, labelAngle=315)).bin(extent=[overallmin, overallmax], step=0.1),
        y=alt.Y('count()', title="number of variants").stack("zero"),
        color=alt.Color('Consequence:N', title="Variant type", scale=alt.Scale(scheme="category10"))
    )

    c = alt.Chart(plotdf, width=550, height=250
            ).mark_bar(
        opacity=0.9,
        binSpacing=0
    ).encode(
        x=alt.X('%s:Q' % scorecol, title="median log2 ratio", 
            axis=alt.Axis(labelFontSize=12, labelAngle=315)).bin(extent=[overallmin, overallmax], step=0.1),
        y=alt.Y('count()', title="Distribution within score bin").stack("normalize"),
        color=alt.Color('Consequence:N', title="Variant type", scale=alt.Scale(scheme="category10"))
    )
    scorechart = (a & (b | c)).properties(
        title="SNV scores for %s at timepoint %s" % (target.targetname, lateday)
    ).configure_title(
        fontSize=20, 
        offset=5, 
        orient='top', 
        anchor='middle',
    )
    scorechart.save(args.snvfig)
    return


def scoreDels(target, args):
    '''compute scores for the 3bp programmed deletions
    
    '''
    # load the day 0 library
    delfiles = target.getDelSampleList(args.countsdir,
                                    include_neg=False)
    libfile = delfiles['D00'][0]
    df = sge_counts.getDelCounts(libfile,
                                 augment=True,
                                 pseudocount=args.pseudocount)
    df = df.rename(columns={'count': 'dellib_count'}
        ).drop(columns=["sampleid", "repl", "day"])
    statsfile = libfile.replace(".dels.tsv", ".readstats.tsv")
    statsdf = sge_counts.getReadStats(statsfile, augment=False)
    normval = statsdf["deletion_reads"][0]
    df['dellib_freq'] = df['dellib_count'] / normval

    if args.verbose:
        print("INFO: Loaded data from deletion Day 0 library")

    # load the various timepoints and replicates
    nsamples = 1
    allrepls = {}

    for delday, delfilelist in sorted(delfiles.items()):
        if delday == "D00": continue  # already loaded
        allrepls[delday] = []
        for delfile in sorted(delfilelist):
            # exclude neg
            if "_NC_" in delfile:
                continue
            nsamples += 1
            if args.verbose:
                print("INFO: loading data from %s" % delfile)
            tmpdf = sge_counts.getDelCounts(delfile,
                                            augment=False,
                                            pseudocount=args.pseudocount)
            statsfile = delfile.replace(".dels.tsv", ".readstats.tsv")
            statsdf = sge_counts.getReadStats(statsfile, augment=False)
            sampleid = tmpdf["sampleid"][0]
            parts = sampleid.split("_")
            repl = parts[2]
            allrepls[delday].append(repl)
            countcol = "%s_%s" % (delday, repl)
            tmpdf = tmpdf.rename(columns={'count': countcol})
            freqcol = countcol + "_freq"
            normval = statsdf["deletion_reads"][0]
            tmpdf[freqcol] = tmpdf[countcol] / normval
            df = df.merge(tmpdf[["chrom", "start", "end", countcol, freqcol]], on=["chrom", "start", "end"])
    if args.verbose:
        print("INFO: Finished loading data from %d samples" % nsamples)
        print("INFO: Found data from timepoints: ", list(allrepls.keys()))

    df = df[(df["start"] >= target.editstartpos) &
            (df["end"] <= target.editendpos)]

    # annotate with vep preditions
    vepdf = sge_util.getVEPdf(args.delvepfile, type="del")

    df = df.merge(vepdf[["start", "end", "Consequence"]], 
                  on=["start", "end"]) 

    # now we have counts, frequencies, and annotations for each position and each sample.
    # next step is to convert them into scores
    for day, repls in sorted(allrepls.items()):
        for repl in repls:
            freqcol = "%s_%s_freq" % (day, repl)
            ratiocol = "%s_%s_over_lib" % (day, repl)
            df[ratiocol] = df[freqcol] / df['dellib_freq']

    # we also need day 13 (or day 11) vs day 5, per replicate
    days = sorted(allrepls.keys())
    earlyday = days[0]
    lateday = days[1]
    for repl in allrepls[earlyday]:
        if repl in allrepls[lateday]:
            df["%s_%s_over_%s_%s" % (lateday, repl, earlyday, repl)] = \
                df["%s_%s_freq" % (lateday, repl)] / df["%s_%s_freq" % (earlyday, repl)]


    for day, repls in sorted(allrepls.items()):
        replcols = []
        for repl in repls:
            replcols.append("%s_%s_over_lib" % (day, repl))
        df["%s_over_lib_median" % day] = df[replcols].median(axis=1)
        df["%s_log2_median" % day] = np.log2(df["%s_over_lib_median" % day])

    return df


def scoreSNVs(target, args, qcfails):
    '''compute scores for SNVs
    
    '''
    # load the day 0 library
    snvfiles = target.getSNVSampleList(args.countsdir,
                                    include_neg=False)
    libfile = snvfiles['D00'][0]
    df = sge_counts.getSNVCounts(libfile,
                                 augment=True,
                                 pseudocount=args.pseudocount)
    df = df.rename(columns={'count': 'snvlib_count'}
        ).drop(columns=["sampleid", "repl", "day"])
    statsfile = libfile.replace(".snvs.tsv", ".readstats.tsv")
    statsdf = sge_counts.getReadStats(statsfile, augment=False)
    normval = statsdf[args.normcol][0]
    df['snvlib_freq'] = df['snvlib_count'] / normval
        
    if args.verbose:
        print("INFO: Loaded data from SNV Day 0 library")

    # load the various timepoints and replicates
    nsamples = 1
    allrepls = {}

    for snvday, snvfilelist in sorted(snvfiles.items()):
        if snvday == "D00": continue  # already loaded
        allrepls[snvday] = []
        for snvfile in sorted(snvfilelist):
            baserepl = snvfile.replace(".snvs.tsv", "")
            baserepl = "_".join(baserepl.split("_")[:-1])
            if baserepl in qcfails:
                print("INFO: found repl %s in qcfails, skipping" % snvfile)
                continue
            # exclude neg
            if "_NC_" in snvfile:
                continue
            nsamples += 1
            if args.verbose:
                print("INFO: loading data from %s" % snvfile)
            tmpdf = sge_counts.getSNVCounts(snvfile,
                                            augment=False,
                                            pseudocount=args.pseudocount)
            statsfile = snvfile.replace(".snvs.tsv", ".readstats.tsv")
            statsdf = sge_counts.getReadStats(statsfile, augment=False)
            sampleid = tmpdf["sampleid"][0]
            parts = sampleid.split("_")
            repl = parts[2]
            allrepls[snvday].append(repl)
            countcol = "%s_%s" % (snvday, repl)
            tmpdf = tmpdf.rename(columns={'count': countcol})
            freqcol = countcol + "_freq"
            normval = statsdf[args.normcol][0]
            tmpdf[freqcol] = tmpdf[countcol] / normval
            df = df.merge(tmpdf[["chrom", "pos", "allele", countcol, freqcol]], on=["chrom", "pos", "allele"])
            
    if args.verbose:
        print("INFO: Finished loading data from %d samples" % nsamples)
        print("INFO: Found data from timepoints: ", list(allrepls.keys()))

    df = df[(df["pos"] >= target.editstartpos) &
            (df["pos"] <= target.editendpos)]
    # merge with the reference sequence
    df = df.merge(target.refdf, on="pos")
    df = df[df["ref"] != df["allele"]]
    df["pos_id"] = df["pos"].astype(str) + ":" + df["allele"]
    # pivot and annotate with metadata
    df["pam_edit_or_snp"] = False
    df.loc[df["pos"].isin(target.required_edits), "pam_edit_or_snp"] = True
    df.loc[df["pos"].isin(target.skip_pos), "pam_edit_or_snp"] = True
    df = df[df["pam_edit_or_snp"] == False]
    df = df[df["snvlib_count"] >= 100]

    # annotate with vep preditions
    vepdf = sge_util.getVEPdf(args.snvvepfile)
    df = df.merge(vepdf[["pos", "allele", "Consequence"]], 
                  on=["pos", "allele"])
    df['Consequence'] = df['Consequence'].map(lambda x: x.split(",")[0])

    # now we have counts, frequencies, and annotations for each position and each sample.
    # next step is to convert them into scores
    for day, repls in sorted(allrepls.items()):
        for repl in repls:
            freqcol = "%s_%s_freq" % (day, repl)
            ratiocol = "%s_%s_over_lib" % (day, repl)
            df[ratiocol] = df[freqcol] / df['snvlib_freq']
            log2col = "%s_%s_over_lib_log2" % (day, repl)
            df[log2col] = np.log2(df[ratiocol])

    # we also need day 13 (or day 11) vs day 5, per replicate
    earlyday = target.earlyday
    lateday = target.lateday
    for repl in allrepls[earlyday]:
        if repl in allrepls[lateday]:
            df["%s_%s_over_%s_%s" % (lateday, repl, earlyday, repl)] = \
                df["%s_%s_freq" % (lateday, repl)] / df["%s_%s_freq" % (earlyday, repl)]
            df["%s_%s_over_%s_%s_log2" % (lateday, repl, earlyday, repl)] = \
                np.log2(df["%s_%s_over_%s_%s" % (lateday, repl, earlyday, repl)])


    return df


def smoothSNVscores(target, args, df):
    earlyday = target.earlyday
    lateday = target.lateday
    filtercols = [col for col in df.columns if col.startswith("%s_" % earlyday) \
                  and col.endswith("over_lib")]
    cond = (df[filtercols[0]] >= 0.5)
    if len(filtercols) > 1:
        for col in filtercols[1:]:
            cond &= (df[col] >= 0.5)

    fullcount = df.shape[0]
    filtdf = df[cond]
    filtcount = filtdf.shape[0]
    if args.verbose:
        print("INFO: smoothing started with %d variants" % fullcount)
        print("INFO: after filtering with threshold=%f, %d variants remain" % (0.5,
                                                                           filtcount))

    for col in filtercols:
        newcol = col.replace("over_lib", "over_lib_loess")
        xlist = filtdf['pos'].to_numpy()
        ylist = filtdf[col].to_numpy()
        ylist = np.log2(ylist)
        xout, yout, wout = loess.loess_1d.loess_1d(xlist,
                                                   ylist,
                                                   xnew=None,
                                                   frac=0.15)
        mydata = np.array([xout, yout])
        tmpdf = pd.DataFrame(data=mydata.T, columns=["pos", newcol]
            ).drop_duplicates(subset=['pos'], keep='first')
        tmpdf['pos'] = tmpdf['pos'].astype(int)
        df = pd.merge_asof(df, tmpdf, on='pos', direction='nearest')
        log2col = col.replace(earlyday, lateday)
        latelog2 = np.log2(df[log2col].to_numpy())
        latelog2col = log2col.replace("over_lib", "over_lib_log2")
        df[latelog2col] = latelog2
        diffcol = log2col.replace("over_lib", "loess_adjust")
        df[diffcol] = df[latelog2col] - df[newcol]

    return df
        

def normalizeSNVscores(target, args, df):
    lateday = target.lateday
    scorecols = [col for col in df.columns if col.startswith("%s_" % lateday) \
                 and col.endswith("loess_adjust")]
    synmedians = []
    nonmedians = []
    for col in scorecols:
        synmedian = df[df["Consequence"] == "synonymous_variant"][col].median()
        nonmedian = df[df["Consequence"] == "stop_gained"][col].median()
        if args.verbose:
            print("INFO: median synonymous score=%f for %s" % (synmedian, col))
            print("INFO: median nonsense score=%f for %s" % (nonmedian, col))
        synmedians.append(synmedian)
        nonmedians.append(nonmedian)
    mediansynmedian = np.median(synmedians)
    mediannonmedian = np.median(nonmedians)
    mediandist = mediansynmedian - mediannonmedian # distance between medians
    if args.verbose:
        print("INFO: median distance=%f between synonymous and nonsense medians" % (mediandist))

    # now scale scores
    newcols = []
    replindex = 0
    for index, col in enumerate(scorecols):
        replindex += 1
        dist = synmedians[index] - nonmedians[index]
        distratio = dist / mediandist
        if args.verbose:
            print("INFO: distance ratio=%f for %s" % (distratio, col))
        scaledcol = df[col] * 1.0 / distratio
        newsynmedian = synmedians[index] * 1.0 / distratio
        locationshift = newsynmedian - mediansynmedian
        if args.verbose:
            print("INFO: location shift=%f for %s" % (locationshift, col))

        locatedcol = scaledcol - locationshift
        newcolname = "R" + str(replindex) + "_score"
        #newcolname = col.replace("loess_adjust", "scaled")
        df[newcolname] = locatedcol        
        newcols.append(newcolname)
    df["target_score"] = df[newcols].median(axis=1)
    keepcols = ["target", "chrom", "pos", "allele", "gene", "ref", "pos_id", "Consequence"]
    for n in newcols:
        keepcols.append(n)
    keepcols.append("target_score")
    return df[keepcols]


#target  chrom   pos     allele  snvlib_count    gene    snvlib_freq     D05_R1R4        D05_R1R4_freq   D05_R2R5        D05_R2R5_freq   D13_R1R4        D13_R1R4_freq    D13_R2R5        D13_R2R5_freq   ref     pos_id  pam_edit_or_snp Consequence     D05_R1R4_over_lib       D05_R1R4_over_lib_log2  D05_R2R5_over_lib        D05_R2R5_over_lib_log2  D13_R1R4_over_lib       D13_R1R4_over_lib_log2  D13_R2R5_over_lib       D13_R2R5_over_lib_log2  D13_R1R4_over_D05_R1R4   D13_R1R4_over_D05_R1R4_log2     D13_R2R5_over_D05_R2R5  D13_R2R5_over_D05_R2R5_log2     D05_R1R4_over_lib_loess D13_R1R4_loess_adjust   D05_R2R5_over_lib_loess  D13_R2R5_loess_adjust   R1_score        R2_score        target_score


def saveSNVScores(args, scoredf):
    outfile = args.snvtsv
    scoredf.to_csv(outfile, index=False, sep="\t")
    if args.verbose:
        print("INFO: wrote SNV scores to %s" % outfile)
    return


def saveDelScores(args, scoredf):
    outfile = args.deltsv
    scoredf.to_csv(outfile, index=False, sep="\t")
    if args.verbose:
        print("INFO: wrote Deletion scores to %s" % outfile)
    return


def main():
    parser = argparse.ArgumentParser('extract matrix of edits for an SGE sample')
    parser.add_argument('-n', '--targetname', required=True,
                        help="Target name -- must match entry in <targetfile>")
    parser.add_argument('-t', '--targetfile', required=True, 
                        help="File containing list of targets and expected edits")
    parser.add_argument('-s', '--snvtsv', required=False, default="", 
                        help="Output TSV file of SNV scores")
    parser.add_argument('-S', '--snvfig', required=False, default="", 
                        help="Output figure of SNV scores")
    parser.add_argument('-d', '--deltsv', required=False, default="", 
                        help="Output TSV file of deletion scores")
    parser.add_argument('-D', '--delfig', required=False, default="", 
                        help="Output figure of deletion scores")
    parser.add_argument('-c', '--countsdir', required=True, default="",
                        help="Directory containing per-sample SNV counts files")
    parser.add_argument('-V', '--snvvepfile', required=True, default="",
                        help="output from Variant Effect Predictor for this SNV library")
    parser.add_argument('-U', '--delvepfile', required=False, default="",
                        help="output from Variant Effect Predictor for this deletion library")
    parser.add_argument('-p', '--pseudocount', type=int, default=1, required=False,
                        help="add <pseudocount> to any zero-count position")
    parser.add_argument('-N', '--normcol', required=False, default="snv_reads",
                        help="Name of column in readstats file to use for normalization")
    parser.add_argument('-Q', '--qcfail', required=False, default="",
                        help="Path to TSV file containing QC failing replicates in 'repl' column")
    parser.add_argument('-v', '--verbose', required=False, default=False,
                        action="store_true", help="Verbose output")
    args = parser.parse_args()

    target = sge_target.Target(args.targetname, args.targetfile)

    if args.snvfig:
        if not args.snvfig.endswith(".html") and not args.snvfig.endswith(".png"):
            sys.stderr.write("ERROR: SNV figure output file must end in .html or .png\n")
            sys.exit(-1)

    if args.delfig:
        if not args.delfig.endswith(".html") and not args.snvfig.endswith(".png"):
            sys.stderr.write("ERROR: Deletion figure output file must end in .html or .png\n")
            sys.exit(-1)

    if args.qcfail:
        qcdf = pd.read_csv(args.qcfail, header=0, sep="\t")
        qcfails = qcdf["repl"].to_list()
        sys.stderr.write("INFO: found %d replicates to skip in qc fail list\n" % len(qcfails))
    else:
        qcfails = []

    # SNV scoring
    if args.snvtsv:
        scoredf = scoreSNVs(target, args, qcfails)
        scoredf = smoothSNVscores(target, args, scoredf)
        scoredf = normalizeSNVscores(target, args, scoredf)
        saveSNVScores(args, scoredf)
        if args.snvfig:
            saveSNVScoreFigure(target, args, scoredf)
    
    if args.deltsv:
        scoredf = scoreDels(target, args)
        saveDelScores(args, scoredf)
        if args.delfig:
            saveDelScoreFigure(target, args, scoredf)

    return 0


if __name__ == '__main__':
    main()
    
